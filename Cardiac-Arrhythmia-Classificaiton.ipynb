{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cardiac Arrhythmia Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello and welcome to my first Github machine learning project! Here we will be building some models to predict whether a person has some type of cardiac arrhythmia. The dataset has 452 records and 280 features, which means we do not have enough records to build a high-performance model. But this shouldn't stop us from building and tuning the best possible model for this dataset.  \n",
    "  \n",
    "From the dataset documentation we know that our label has several classes, some of which having zero instances in the dataset. This is not a problem, as we will assume that these classes do not exist.  \n",
    "  \n",
    "The following will be our strategy for this project:  \n",
    "- Build simple classification models using KNN, Logistic Regression, SVM, Decision Trees, Random Forests. We will use GridSearch to tune each model's parameters, which also implements cross validation.\n",
    "- Pick a set of parameters for each model that has low bias but high variance (aka overfitting, train score will be higher than test score). Perform bagging methods on these models. Then pick a set of parameters for each model that has high bias but low variance (in other words, train and test scores will be similar but the accuracy might be low). Perform boosting methods on them. The goal here is to see which method gives the best performance on each model.\n",
    "- Use principal components analysis (PCA) to reduce the number of features while retaining as much information as possible, then build simple models to judge computation time and model performance.\n",
    "- Choose the best model (simple, with bagging/boosting, or with PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import major libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardiac = pd.read_csv('cardiac_arrhythmia.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   270   271  272  273  \\\n",
       "0   75    0  190   80   91  193  371  174  121  -16 ...   0.0   9.0 -0.9  0.0   \n",
       "1   56    1  165   64   81  174  401  149   39   25 ...   0.0   8.5  0.0  0.0   \n",
       "2   54    0  172   95  138  163  386  185  102   96 ...   0.0   9.5 -2.4  0.0   \n",
       "3   55    0  175   94  100  202  380  179  143   28 ...   0.0  12.2 -2.2  0.0   \n",
       "4   75    0  190   80   88  181  360  177  103  -16 ...   0.0  13.1 -3.6  0.0   \n",
       "\n",
       "  274  275  276   277   278  279  \n",
       "0   0  0.9  2.9  23.3  49.4    8  \n",
       "1   0  0.2  2.1  20.4  38.8    6  \n",
       "2   0  0.3  3.4  12.3  49.0   10  \n",
       "3   0  0.4  2.6  34.6  61.6    1  \n",
       "4   0 -0.1  3.9  25.4  62.8    7  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardiac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows and columns of the dataset: (452, 280)\n"
     ]
    }
   ],
   "source": [
    "print('Rows and columns of the dataset:', cardiac.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace '?' with NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardiac.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check which columns have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Number of Nulls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Column  Number of Nulls\n",
       "10      10                8\n",
       "11      11               22\n",
       "12      12                1\n",
       "13      13              376\n",
       "14      14                1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nulls = cardiac.isnull().sum().reset_index()\n",
    "nulls.columns = ['Column', 'Number of Nulls']\n",
    "nulls = nulls.loc[nulls['Number of Nulls'] != 0]\n",
    "nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 376 values are missing in column 13 (Vector angles in degrees on front plane of QRST), which is 83% of the data. This column will have to be dropped since it has too many missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cardiac.drop(columns=13, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the columns, replace missing values with mean of that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in [10,11,12,14]:\n",
    "    cardiac[i].fillna(cardiac[i].astype(float).mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162   -0.197555\n",
       "242   -0.189458\n",
       "1     -0.178080\n",
       "168   -0.171763\n",
       "270   -0.164321\n",
       "260   -0.162153\n",
       "247   -0.159612\n",
       "172   -0.158536\n",
       "252   -0.150610\n",
       "202   -0.142731\n",
       "161   -0.135180\n",
       "208   -0.134657\n",
       "8     -0.122003\n",
       "89    -0.118168\n",
       "237   -0.108902\n",
       "165   -0.105922\n",
       "167   -0.102059\n",
       "5     -0.099954\n",
       "212   -0.099329\n",
       "238   -0.094543\n",
       "234   -0.093912\n",
       "214   -0.093510\n",
       "230   -0.092791\n",
       "0     -0.092381\n",
       "3     -0.090151\n",
       "255   -0.086873\n",
       "275   -0.086427\n",
       "206   -0.084941\n",
       "48    -0.084021\n",
       "257   -0.083396\n",
       "         ...   \n",
       "229    0.119809\n",
       "88     0.120726\n",
       "107    0.121711\n",
       "193    0.122928\n",
       "249    0.124823\n",
       "198    0.125873\n",
       "65     0.127210\n",
       "228    0.128490\n",
       "149    0.130056\n",
       "181    0.130360\n",
       "119    0.132195\n",
       "113    0.140502\n",
       "56     0.141103\n",
       "221    0.141274\n",
       "152    0.141506\n",
       "77     0.143284\n",
       "239    0.151782\n",
       "68     0.152534\n",
       "191    0.165693\n",
       "125    0.170670\n",
       "52     0.173243\n",
       "94     0.174346\n",
       "29     0.183083\n",
       "17     0.195198\n",
       "233    0.218811\n",
       "223    0.235488\n",
       "102    0.282523\n",
       "92     0.313982\n",
       "4      0.323879\n",
       "90     0.368876\n",
       "Name: 279, Length: 257, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cardiac.corr()[cardiac.shape[1]].sort_values().head(257)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the columns with the most positive and negative correlations with our target variable. We can see that no strong correlations exist between the target variable and any other column in the data. This means that we should not expect a large accuracy even with the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore distribution of the target variable (column 279)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0,'Arrhythmia')"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtcAAAF3CAYAAABuemcuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGES5BgCAQZRrAAAYRLkGAIBB/h/67PKuY5OJmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2be025af2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(x=279, data = cardiac)\n",
    "plt.xlabel('Arrhythmia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that approximately half of the samples belong to the 'Normal' class. This imbalance will bias the results of any learning model. Therefore, bagging and boosting methods are required to make the most reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = cardiac.iloc[:, 0:278]\n",
    "y = cardiac.iloc[:, 278]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler \n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classification models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "kFold = StratifiedKFold(n_splits=8)\n",
    "\n",
    "knn = KNeighborsClassifier(n_jobs=-1)\n",
    "params = {'n_neighbors': [1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "knn_grid = GridSearchCV(estimator=knn, param_grid=params, n_jobs=-1, cv=kFold)\n",
    "knn_grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_n_neighbors</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.588608</td>\n",
       "      <td>0.625246</td>\n",
       "      <td>5</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.634686</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.629496</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.622776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.037901</td>\n",
       "      <td>0.005459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.121129</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.692148</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_neighbors': 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.682657</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.669039</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.062175</td>\n",
       "      <td>0.011987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115134</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.639724</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_neighbors': 4}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.649446</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.629893</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.044917</td>\n",
       "      <td>0.007451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117177</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.603534</td>\n",
       "      <td>6</td>\n",
       "      <td>{'n_neighbors': 6}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.608856</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.607914</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.600719</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.003681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114105</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.599937</td>\n",
       "      <td>7</td>\n",
       "      <td>{'n_neighbors': 7}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.605166</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.589928</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.593525</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.601423</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006641</td>\n",
       "      <td>0.045802</td>\n",
       "      <td>0.006345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "4       0.000000         0.117873         0.588608          0.625246   \n",
       "1       0.003902         0.121129         0.582278          0.692148   \n",
       "3       0.000000         0.115134         0.582278          0.639724   \n",
       "5       0.000000         0.117177         0.582278          0.603534   \n",
       "6       0.000000         0.114105         0.582278          0.599937   \n",
       "\n",
       "  param_n_neighbors              params  rank_test_score  split0_test_score  \\\n",
       "4                 5  {'n_neighbors': 5}                1           0.533333   \n",
       "1                 2  {'n_neighbors': 2}                2           0.511111   \n",
       "3                 4  {'n_neighbors': 4}                2           0.533333   \n",
       "5                 6  {'n_neighbors': 6}                2           0.533333   \n",
       "6                 7  {'n_neighbors': 7}                2           0.511111   \n",
       "\n",
       "   split0_train_score  split1_test_score       ...         split5_test_score  \\\n",
       "4            0.634686           0.560976       ...                  0.631579   \n",
       "1            0.682657           0.560976       ...                  0.631579   \n",
       "3            0.649446           0.512195       ...                  0.605263   \n",
       "5            0.608856           0.512195       ...                  0.631579   \n",
       "6            0.605166           0.536585       ...                  0.631579   \n",
       "\n",
       "   split5_train_score  split6_test_score  split6_train_score  \\\n",
       "4            0.622302           0.578947            0.629496   \n",
       "1            0.712230           0.500000            0.694245   \n",
       "3            0.633094           0.578947            0.643885   \n",
       "5            0.607914           0.605263            0.600719   \n",
       "6            0.589928           0.605263            0.593525   \n",
       "\n",
       "   split7_test_score  split7_train_score  std_fit_time  std_score_time  \\\n",
       "4           0.657143            0.622776      0.000000        0.003191   \n",
       "1           0.685714            0.669039      0.006758        0.008464   \n",
       "3           0.657143            0.629893      0.000000        0.007681   \n",
       "5           0.628571            0.601423      0.000000        0.007804   \n",
       "6           0.657143            0.601423      0.000000        0.006641   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "4        0.037901         0.005459  \n",
       "1        0.062175         0.011987  \n",
       "3        0.044917         0.007451  \n",
       "5        0.041638         0.003681  \n",
       "6        0.045802         0.006345  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid_results = pd.DataFrame(knn_grid.cv_results_).sort_values('rank_test_score')\n",
    "knn_grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 3 neighbors are the best parameters for this learning model using the elbow method below. However, this model overfits fits the data because of the difference in train and test core. Also, the train and test scores are still unacceptable. The 2nd ranking model (k = 6) would probably be the better choice. <br>The code below plots the error rate for every value of k between 1 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Error Rate')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAGDCAYAAAB0s1eWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKKoHQ468AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2be68686e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "error_rate = []\n",
    "\n",
    "for i in range(1,10):\n",
    "    \n",
    "    knn2 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn2.fit(X_train,y_train)\n",
    "    pred_i = knn2.predict(X_test)\n",
    "    error_rate.append(np.mean(pred_i != y_test))\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,10),error_rate,color='blue', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='red', markersize=10)\n",
    "plt.title('Error Rate vs. K Value')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Error Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "params = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "LR_grid = GridSearchCV(estimator=LR, param_grid=params, cv=kFold)\n",
    "LR_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.088138</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.702532</td>\n",
       "      <td>0.850367</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.856089</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.838129</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.850534</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.067706</td>\n",
       "      <td>0.008248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.083177</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.686709</td>\n",
       "      <td>0.967442</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.971223</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.971530</td>\n",
       "      <td>0.015225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054737</td>\n",
       "      <td>0.006029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.084620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.674051</td>\n",
       "      <td>0.999550</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060608</td>\n",
       "      <td>0.001190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.080079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.604430</td>\n",
       "      <td>0.645156</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.649446</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.640288</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.637011</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050284</td>\n",
       "      <td>0.005168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.079257</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'C': 1e-05}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.568266</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "5       0.088138         0.000250         0.702532          0.850367       1   \n",
       "6       0.083177         0.000000         0.686709          0.967442      10   \n",
       "7       0.084620         0.000000         0.674051          0.999550     100   \n",
       "4       0.080079         0.000000         0.604430          0.645156     0.1   \n",
       "0       0.079257         0.001953         0.556962          0.557019   1e-05   \n",
       "\n",
       "         params  rank_test_score  split0_test_score  split0_train_score  \\\n",
       "5      {'C': 1}                1           0.688889            0.856089   \n",
       "6     {'C': 10}                2           0.666667            0.963100   \n",
       "7    {'C': 100}                3           0.688889            1.000000   \n",
       "4    {'C': 0.1}                4           0.533333            0.649446   \n",
       "0  {'C': 1e-05}                5           0.488889            0.568266   \n",
       "\n",
       "   split1_test_score       ...         split5_test_score  split5_train_score  \\\n",
       "5           0.682927       ...                  0.710526            0.856115   \n",
       "6           0.682927       ...                  0.631579            0.964029   \n",
       "7           0.609756       ...                  0.605263            1.000000   \n",
       "4           0.560976       ...                  0.631579            0.643885   \n",
       "0           0.536585       ...                  0.578947            0.553957   \n",
       "\n",
       "   split6_test_score  split6_train_score  split7_test_score  \\\n",
       "5           0.684211            0.838129           0.885714   \n",
       "6           0.684211            0.971223           0.828571   \n",
       "7           0.657895            1.000000           0.800000   \n",
       "4           0.605263            0.640288           0.714286   \n",
       "0           0.578947            0.553957           0.628571   \n",
       "\n",
       "   split7_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "5            0.850534      0.011317        0.000433        0.067706   \n",
       "6            0.971530      0.015225        0.000000        0.054737   \n",
       "7            1.000000      0.010446        0.000000        0.060608   \n",
       "4            0.637011      0.005167        0.000000        0.050284   \n",
       "0            0.548043      0.006575        0.005168        0.038937   \n",
       "\n",
       "   std_train_score  \n",
       "5         0.008248  \n",
       "6         0.006029  \n",
       "7         0.001190  \n",
       "4         0.005168  \n",
       "0         0.005633  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_grid_results = pd.DataFrame(LR_grid.cv_results_).sort_values('rank_test_score')\n",
    "LR_grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean train score is good, but the 20% difference between mean test and train scores implies that the model is overfitting. We select the model with the highest possible test score, with the smallest possible variance between test and train scores. This corresponds to C = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC = LinearSVC(multi_class='crammer_singer')\n",
    "\n",
    "params = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "LSVC_grid = GridSearchCV(estimator=LSVC, param_grid=params, n_jobs=-1, cv=kFold)\n",
    "LSVC_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.976402</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.721519</td>\n",
       "      <td>0.936706</td>\n",
       "      <td>1</td>\n",
       "      <td>{'C': 1}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.938849</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.938849</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.939502</td>\n",
       "      <td>0.432839</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.074758</td>\n",
       "      <td>0.004063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.650399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.712025</td>\n",
       "      <td>0.808737</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'C': 0.1}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.804428</td>\n",
       "      <td>0.682927</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.812950</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.812950</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.814947</td>\n",
       "      <td>0.135079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.052037</td>\n",
       "      <td>0.004847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.369981</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.639241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.609756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350442</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.034998</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.594407</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>0.636076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>{'C': 10}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.755496</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.034705</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.248050</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.575949</td>\n",
       "      <td>0.598111</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'C': 0.01}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.601476</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.600719</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.582734</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.063680</td>\n",
       "      <td>0.009367</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>0.006782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "5       1.976402         0.003906         0.721519          0.936706       1   \n",
       "4       0.650399         0.000000         0.712025          0.808737     0.1   \n",
       "7       4.369981         0.001953         0.639241          1.000000     100   \n",
       "6       4.594407         0.000751         0.636076          1.000000      10   \n",
       "3       0.248050         0.013672         0.575949          0.598111    0.01   \n",
       "\n",
       "        params  rank_test_score  split0_test_score  split0_train_score  \\\n",
       "5     {'C': 1}                1           0.733333            0.940959   \n",
       "4   {'C': 0.1}                2           0.644444            0.804428   \n",
       "7   {'C': 100}                3           0.622222            1.000000   \n",
       "6    {'C': 10}                4           0.622222            1.000000   \n",
       "3  {'C': 0.01}                5           0.511111            0.601476   \n",
       "\n",
       "   split1_test_score       ...         split5_test_score  split5_train_score  \\\n",
       "5           0.731707       ...                  0.657895            0.938849   \n",
       "4           0.682927       ...                  0.710526            0.812950   \n",
       "7           0.609756       ...                  0.684211            1.000000   \n",
       "6           0.658537       ...                  0.684211            1.000000   \n",
       "3           0.536585       ...                  0.578947            0.600719   \n",
       "\n",
       "   split6_test_score  split6_train_score  split7_test_score  \\\n",
       "5           0.605263            0.938849           0.885714   \n",
       "4           0.710526            0.812950           0.828571   \n",
       "7           0.605263            1.000000           0.685714   \n",
       "6           0.578947            1.000000           0.685714   \n",
       "3           0.605263            0.582734           0.628571   \n",
       "\n",
       "   split7_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "5            0.939502      0.432839        0.006766        0.074758   \n",
       "4            0.814947      0.135079        0.000000        0.052037   \n",
       "7            1.000000      0.350442        0.005168        0.034998   \n",
       "6            1.000000      0.755496        0.000434        0.034705   \n",
       "3            0.597865      0.063680        0.009367        0.040152   \n",
       "\n",
       "   std_train_score  \n",
       "5         0.004063  \n",
       "4         0.004847  \n",
       "7         0.000000  \n",
       "6         0.000000  \n",
       "3         0.006782  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC_grid_results = pd.DataFrame(LSVC_grid.cv_results_).sort_values('rank_test_score')\n",
    "LSVC_grid_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has some bias and variance due to difference between mean train and test scores. This model overfits. We select the model with the highest possible test score, with the smallest possible variance between test and train scores. This corresponds to C = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=OneVsOneClassifier(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "          n_jobs=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'estimator__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'estimator__gamma': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], 'estimator__kernel': ['poly']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "SVC = SVC()\n",
    "OVO_SVC = OneVsOneClassifier(estimator=SVC)\n",
    "\n",
    "params = {'estimator__C': [0.0001,0.001,0.01,0.1,1,10,100],\n",
    "          'estimator__gamma': [0.00001,0.0001,0.001,0.01,0.1,1,10,100],\n",
    "          'estimator__kernel': ['poly']}\n",
    "SVC_grid = GridSearchCV(estimator=OVO_SVC, param_grid=params, cv=kFold)\n",
    "SVC_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>param_estimator__gamma</th>\n",
       "      <th>param_estimator__kernel</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.905972</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 0.01...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.902878</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.910072</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>6.502099e-07</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>0.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.031248</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.905972</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.902878</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.910072</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>5.033594e-06</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>0.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.071247</td>\n",
       "      <td>0.029295</td>\n",
       "      <td>0.718354</td>\n",
       "      <td>0.905972</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 0.1,...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.902878</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.910072</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.900356</td>\n",
       "      <td>0.011687</td>\n",
       "      <td>5.173769e-03</td>\n",
       "      <td>0.053006</td>\n",
       "      <td>0.003325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.070314</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 100,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>4.906081e-07</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.070311</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 100...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007815</td>\n",
       "      <td>4.607327e-07</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.070314</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 10,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>5.167483e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.070313</td>\n",
       "      <td>0.031249</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 1, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>5.230881e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.076173</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 10, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>5.167934e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.072793</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 1, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006992</td>\n",
       "      <td>8.311162e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.066409</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 0.1, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>5.167393e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.071345</td>\n",
       "      <td>0.031472</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 0.1, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009199</td>\n",
       "      <td>5.862014e-04</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.068358</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 10...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>5.789250e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 10...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007811</td>\n",
       "      <td>2.384186e-07</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.033204</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 1, 'e...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011050</td>\n",
       "      <td>5.167280e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 10, 'e...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>7.812619e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 10, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>5.167404e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.068361</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 100, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>1.014593e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 10, ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>5.167292e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.072269</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 100,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007562</td>\n",
       "      <td>6.765806e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.066406</td>\n",
       "      <td>0.031253</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010336</td>\n",
       "      <td>5.376904e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.068361</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>10</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 1...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010870</td>\n",
       "      <td>7.881123e-06</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.070314</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 1, 'es...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.167449e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 0.1,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>5.167483e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.068361</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 1, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>6.529362e-07</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 1,...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>2.779776e-07</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 100, '...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007814</td>\n",
       "      <td>7.812560e-03</td>\n",
       "      <td>0.056433</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.072267</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.698017</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 0.01,...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.690391</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>6.765324e-03</td>\n",
       "      <td>0.048789</td>\n",
       "      <td>0.004366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.066407</td>\n",
       "      <td>0.037110</td>\n",
       "      <td>0.620253</td>\n",
       "      <td>0.698017</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 0.1...</td>\n",
       "      <td>27</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.701439</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.694245</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.690391</td>\n",
       "      <td>0.010335</td>\n",
       "      <td>7.564297e-03</td>\n",
       "      <td>0.048789</td>\n",
       "      <td>0.004366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 0.000...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>7.564513e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>10</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 0.001...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>7.564667e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>100</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 1e-0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>6.765875e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 0.00...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>6.765462e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.066409</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>100</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 100, 'estimator__gamma': 0.00...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.010334</td>\n",
       "      <td>6.071195e-07</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>10</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 10, 'estimator__gamma': 1e-05...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>6.765841e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.068360</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 0.01, ...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>7.348554e-07</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.064006</td>\n",
       "      <td>0.021905</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 1...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>6.073389e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.058595</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 0.0001...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006765</td>\n",
       "      <td>7.564059e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060868</td>\n",
       "      <td>0.024986</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005353</td>\n",
       "      <td>7.322045e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061816</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005261</td>\n",
       "      <td>6.095282e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061471</td>\n",
       "      <td>0.027346</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006023</td>\n",
       "      <td>6.762211e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063063</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.0001, 'estimator__gamma': 0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>7.974558e-07</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.058597</td>\n",
       "      <td>0.025389</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 1e...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>7.562737e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.058595</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 0....</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>7.561644e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 0....</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>6.765806e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.062501</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 0....</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>7.812440e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.066410</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.001, 'estimator__gamma': 0....</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>6.765617e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.060545</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 1e-...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>2.308478e-07</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 0.0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>6.765497e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.059556</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 0.0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.009526</td>\n",
       "      <td>7.564367e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.060550</td>\n",
       "      <td>0.027346</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.01, 'estimator__gamma': 0.0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>6.766944e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.060552</td>\n",
       "      <td>0.029295</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 1e-0...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005163</td>\n",
       "      <td>5.166697e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.060546</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 0.00...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>6.765944e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.060517</td>\n",
       "      <td>0.023436</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 0.00...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005157</td>\n",
       "      <td>7.810445e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.060548</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 1e-05,...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>5.167404e-03</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.057487</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 1, 'estimator__gamma': 0.001,...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>3.153981e-07</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.065966</td>\n",
       "      <td>0.031072</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557019</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>poly</td>\n",
       "      <td>{'estimator__C': 0.1, 'estimator__gamma': 0.01...</td>\n",
       "      <td>29</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>4.731989e-04</td>\n",
       "      <td>0.038937</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "51       0.072267         0.031251         0.718354          0.905972   \n",
       "5        0.072267         0.031248         0.718354          0.905972   \n",
       "28       0.071247         0.029295         0.718354          0.905972   \n",
       "55       0.070314         0.031251         0.680380          1.000000   \n",
       "23       0.070311         0.031251         0.680380          1.000000   \n",
       "22       0.070314         0.029297         0.680380          1.000000   \n",
       "21       0.070313         0.031249         0.680380          1.000000   \n",
       "54       0.076173         0.033204         0.680380          1.000000   \n",
       "29       0.072793         0.031251         0.680380          1.000000   \n",
       "36       0.066409         0.029297         0.680380          1.000000   \n",
       "44       0.071345         0.031472         0.680380          1.000000   \n",
       "15       0.068358         0.031253         0.680380          1.000000   \n",
       "14       0.070316         0.031250         0.680380          1.000000   \n",
       "45       0.070316         0.033204         0.680380          1.000000   \n",
       "38       0.072267         0.031251         0.680380          1.000000   \n",
       "46       0.072267         0.029297         0.680380          1.000000   \n",
       "47       0.068361         0.031251         0.680380          1.000000   \n",
       "30       0.070316         0.029297         0.680380          1.000000   \n",
       "31       0.072269         0.027344         0.680380          1.000000   \n",
       "7        0.066406         0.031253         0.680380          1.000000   \n",
       "6        0.068361         0.031251         0.680380          1.000000   \n",
       "37       0.070314         0.029298         0.680380          1.000000   \n",
       "52       0.072267         0.029298         0.680380          1.000000   \n",
       "53       0.068361         0.031251         0.680380          1.000000   \n",
       "13       0.072267         0.031250         0.680380          1.000000   \n",
       "39       0.070312         0.031250         0.680380          1.000000   \n",
       "43       0.072267         0.035157         0.620253          0.698017   \n",
       "20       0.066407         0.037110         0.620253          0.698017   \n",
       "41       0.060548         0.021485         0.556962          0.557019   \n",
       "42       0.060548         0.025391         0.556962          0.557019   \n",
       "48       0.060548         0.027344         0.556962          0.557019   \n",
       "49       0.060548         0.027344         0.556962          0.557019   \n",
       "50       0.066409         0.031251         0.556962          0.557019   \n",
       "40       0.060548         0.027344         0.556962          0.557019   \n",
       "35       0.068360         0.031251         0.556962          0.557019   \n",
       "0        0.064006         0.021905         0.556962          0.557019   \n",
       "33       0.058595         0.021485         0.556962          0.557019   \n",
       "1        0.060868         0.024986         0.556962          0.557019   \n",
       "2        0.061816         0.027054         0.556962          0.557019   \n",
       "3        0.061471         0.027346         0.556962          0.557019   \n",
       "4        0.063063         0.031251         0.556962          0.557019   \n",
       "8        0.058597         0.025389         0.556962          0.557019   \n",
       "9        0.058595         0.025393         0.556962          0.557019   \n",
       "10       0.060548         0.027344         0.556962          0.557019   \n",
       "11       0.062501         0.023438         0.556962          0.557019   \n",
       "12       0.066410         0.035157         0.556962          0.557019   \n",
       "16       0.060545         0.031250         0.556962          0.557019   \n",
       "17       0.060548         0.027344         0.556962          0.557019   \n",
       "18       0.059556         0.025391         0.556962          0.557019   \n",
       "19       0.060550         0.027346         0.556962          0.557019   \n",
       "24       0.060552         0.029295         0.556962          0.557019   \n",
       "25       0.060546         0.027344         0.556962          0.557019   \n",
       "26       0.060517         0.023436         0.556962          0.557019   \n",
       "32       0.060548         0.029298         0.556962          0.557019   \n",
       "34       0.057487         0.031250         0.556962          0.557019   \n",
       "27       0.065966         0.031072         0.556962          0.557019   \n",
       "\n",
       "   param_estimator__C param_estimator__gamma param_estimator__kernel  \\\n",
       "51                100                   0.01                    poly   \n",
       "5              0.0001                      1                    poly   \n",
       "28                0.1                    0.1                    poly   \n",
       "55                100                    100                    poly   \n",
       "23               0.01                    100                    poly   \n",
       "22               0.01                     10                    poly   \n",
       "21               0.01                      1                    poly   \n",
       "54                100                     10                    poly   \n",
       "29                0.1                      1                    poly   \n",
       "36                  1                    0.1                    poly   \n",
       "44                 10                    0.1                    poly   \n",
       "15              0.001                    100                    poly   \n",
       "14              0.001                     10                    poly   \n",
       "45                 10                      1                    poly   \n",
       "38                  1                     10                    poly   \n",
       "46                 10                     10                    poly   \n",
       "47                 10                    100                    poly   \n",
       "30                0.1                     10                    poly   \n",
       "31                0.1                    100                    poly   \n",
       "7              0.0001                    100                    poly   \n",
       "6              0.0001                     10                    poly   \n",
       "37                  1                      1                    poly   \n",
       "52                100                    0.1                    poly   \n",
       "53                100                      1                    poly   \n",
       "13              0.001                      1                    poly   \n",
       "39                  1                    100                    poly   \n",
       "43                 10                   0.01                    poly   \n",
       "20               0.01                    0.1                    poly   \n",
       "41                 10                 0.0001                    poly   \n",
       "42                 10                  0.001                    poly   \n",
       "48                100                  1e-05                    poly   \n",
       "49                100                 0.0001                    poly   \n",
       "50                100                  0.001                    poly   \n",
       "40                 10                  1e-05                    poly   \n",
       "35                  1                   0.01                    poly   \n",
       "0              0.0001                  1e-05                    poly   \n",
       "33                  1                 0.0001                    poly   \n",
       "1              0.0001                 0.0001                    poly   \n",
       "2              0.0001                  0.001                    poly   \n",
       "3              0.0001                   0.01                    poly   \n",
       "4              0.0001                    0.1                    poly   \n",
       "8               0.001                  1e-05                    poly   \n",
       "9               0.001                 0.0001                    poly   \n",
       "10              0.001                  0.001                    poly   \n",
       "11              0.001                   0.01                    poly   \n",
       "12              0.001                    0.1                    poly   \n",
       "16               0.01                  1e-05                    poly   \n",
       "17               0.01                 0.0001                    poly   \n",
       "18               0.01                  0.001                    poly   \n",
       "19               0.01                   0.01                    poly   \n",
       "24                0.1                  1e-05                    poly   \n",
       "25                0.1                 0.0001                    poly   \n",
       "26                0.1                  0.001                    poly   \n",
       "32                  1                  1e-05                    poly   \n",
       "34                  1                  0.001                    poly   \n",
       "27                0.1                   0.01                    poly   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "51  {'estimator__C': 100, 'estimator__gamma': 0.01...                1   \n",
       "5   {'estimator__C': 0.0001, 'estimator__gamma': 1...                1   \n",
       "28  {'estimator__C': 0.1, 'estimator__gamma': 0.1,...                1   \n",
       "55  {'estimator__C': 100, 'estimator__gamma': 100,...                4   \n",
       "23  {'estimator__C': 0.01, 'estimator__gamma': 100...                4   \n",
       "22  {'estimator__C': 0.01, 'estimator__gamma': 10,...                4   \n",
       "21  {'estimator__C': 0.01, 'estimator__gamma': 1, ...                4   \n",
       "54  {'estimator__C': 100, 'estimator__gamma': 10, ...                4   \n",
       "29  {'estimator__C': 0.1, 'estimator__gamma': 1, '...                4   \n",
       "36  {'estimator__C': 1, 'estimator__gamma': 0.1, '...                4   \n",
       "44  {'estimator__C': 10, 'estimator__gamma': 0.1, ...                4   \n",
       "15  {'estimator__C': 0.001, 'estimator__gamma': 10...                4   \n",
       "14  {'estimator__C': 0.001, 'estimator__gamma': 10...                4   \n",
       "45  {'estimator__C': 10, 'estimator__gamma': 1, 'e...                4   \n",
       "38  {'estimator__C': 1, 'estimator__gamma': 10, 'e...                4   \n",
       "46  {'estimator__C': 10, 'estimator__gamma': 10, '...                4   \n",
       "47  {'estimator__C': 10, 'estimator__gamma': 100, ...                4   \n",
       "30  {'estimator__C': 0.1, 'estimator__gamma': 10, ...                4   \n",
       "31  {'estimator__C': 0.1, 'estimator__gamma': 100,...                4   \n",
       "7   {'estimator__C': 0.0001, 'estimator__gamma': 1...                4   \n",
       "6   {'estimator__C': 0.0001, 'estimator__gamma': 1...                4   \n",
       "37  {'estimator__C': 1, 'estimator__gamma': 1, 'es...                4   \n",
       "52  {'estimator__C': 100, 'estimator__gamma': 0.1,...                4   \n",
       "53  {'estimator__C': 100, 'estimator__gamma': 1, '...                4   \n",
       "13  {'estimator__C': 0.001, 'estimator__gamma': 1,...                4   \n",
       "39  {'estimator__C': 1, 'estimator__gamma': 100, '...                4   \n",
       "43  {'estimator__C': 10, 'estimator__gamma': 0.01,...               27   \n",
       "20  {'estimator__C': 0.01, 'estimator__gamma': 0.1...               27   \n",
       "41  {'estimator__C': 10, 'estimator__gamma': 0.000...               29   \n",
       "42  {'estimator__C': 10, 'estimator__gamma': 0.001...               29   \n",
       "48  {'estimator__C': 100, 'estimator__gamma': 1e-0...               29   \n",
       "49  {'estimator__C': 100, 'estimator__gamma': 0.00...               29   \n",
       "50  {'estimator__C': 100, 'estimator__gamma': 0.00...               29   \n",
       "40  {'estimator__C': 10, 'estimator__gamma': 1e-05...               29   \n",
       "35  {'estimator__C': 1, 'estimator__gamma': 0.01, ...               29   \n",
       "0   {'estimator__C': 0.0001, 'estimator__gamma': 1...               29   \n",
       "33  {'estimator__C': 1, 'estimator__gamma': 0.0001...               29   \n",
       "1   {'estimator__C': 0.0001, 'estimator__gamma': 0...               29   \n",
       "2   {'estimator__C': 0.0001, 'estimator__gamma': 0...               29   \n",
       "3   {'estimator__C': 0.0001, 'estimator__gamma': 0...               29   \n",
       "4   {'estimator__C': 0.0001, 'estimator__gamma': 0...               29   \n",
       "8   {'estimator__C': 0.001, 'estimator__gamma': 1e...               29   \n",
       "9   {'estimator__C': 0.001, 'estimator__gamma': 0....               29   \n",
       "10  {'estimator__C': 0.001, 'estimator__gamma': 0....               29   \n",
       "11  {'estimator__C': 0.001, 'estimator__gamma': 0....               29   \n",
       "12  {'estimator__C': 0.001, 'estimator__gamma': 0....               29   \n",
       "16  {'estimator__C': 0.01, 'estimator__gamma': 1e-...               29   \n",
       "17  {'estimator__C': 0.01, 'estimator__gamma': 0.0...               29   \n",
       "18  {'estimator__C': 0.01, 'estimator__gamma': 0.0...               29   \n",
       "19  {'estimator__C': 0.01, 'estimator__gamma': 0.0...               29   \n",
       "24  {'estimator__C': 0.1, 'estimator__gamma': 1e-0...               29   \n",
       "25  {'estimator__C': 0.1, 'estimator__gamma': 0.00...               29   \n",
       "26  {'estimator__C': 0.1, 'estimator__gamma': 0.00...               29   \n",
       "32  {'estimator__C': 1, 'estimator__gamma': 1e-05,...               29   \n",
       "34  {'estimator__C': 1, 'estimator__gamma': 0.001,...               29   \n",
       "27  {'estimator__C': 0.1, 'estimator__gamma': 0.01...               29   \n",
       "\n",
       "    split0_test_score       ...         split5_test_score  split5_train_score  \\\n",
       "51           0.711111       ...                  0.684211            0.902878   \n",
       "5            0.711111       ...                  0.684211            0.902878   \n",
       "28           0.711111       ...                  0.684211            0.902878   \n",
       "55           0.688889       ...                  0.605263            1.000000   \n",
       "23           0.688889       ...                  0.605263            1.000000   \n",
       "22           0.688889       ...                  0.605263            1.000000   \n",
       "21           0.688889       ...                  0.605263            1.000000   \n",
       "54           0.688889       ...                  0.605263            1.000000   \n",
       "29           0.688889       ...                  0.605263            1.000000   \n",
       "36           0.688889       ...                  0.605263            1.000000   \n",
       "44           0.688889       ...                  0.605263            1.000000   \n",
       "15           0.688889       ...                  0.605263            1.000000   \n",
       "14           0.688889       ...                  0.605263            1.000000   \n",
       "45           0.688889       ...                  0.605263            1.000000   \n",
       "38           0.688889       ...                  0.605263            1.000000   \n",
       "46           0.688889       ...                  0.605263            1.000000   \n",
       "47           0.688889       ...                  0.605263            1.000000   \n",
       "30           0.688889       ...                  0.605263            1.000000   \n",
       "31           0.688889       ...                  0.605263            1.000000   \n",
       "7            0.688889       ...                  0.605263            1.000000   \n",
       "6            0.688889       ...                  0.605263            1.000000   \n",
       "37           0.688889       ...                  0.605263            1.000000   \n",
       "52           0.688889       ...                  0.605263            1.000000   \n",
       "53           0.688889       ...                  0.605263            1.000000   \n",
       "13           0.688889       ...                  0.605263            1.000000   \n",
       "39           0.688889       ...                  0.605263            1.000000   \n",
       "43           0.533333       ...                  0.657895            0.701439   \n",
       "20           0.533333       ...                  0.657895            0.701439   \n",
       "41           0.488889       ...                  0.578947            0.553957   \n",
       "42           0.488889       ...                  0.578947            0.553957   \n",
       "48           0.488889       ...                  0.578947            0.553957   \n",
       "49           0.488889       ...                  0.578947            0.553957   \n",
       "50           0.488889       ...                  0.578947            0.553957   \n",
       "40           0.488889       ...                  0.578947            0.553957   \n",
       "35           0.488889       ...                  0.578947            0.553957   \n",
       "0            0.488889       ...                  0.578947            0.553957   \n",
       "33           0.488889       ...                  0.578947            0.553957   \n",
       "1            0.488889       ...                  0.578947            0.553957   \n",
       "2            0.488889       ...                  0.578947            0.553957   \n",
       "3            0.488889       ...                  0.578947            0.553957   \n",
       "4            0.488889       ...                  0.578947            0.553957   \n",
       "8            0.488889       ...                  0.578947            0.553957   \n",
       "9            0.488889       ...                  0.578947            0.553957   \n",
       "10           0.488889       ...                  0.578947            0.553957   \n",
       "11           0.488889       ...                  0.578947            0.553957   \n",
       "12           0.488889       ...                  0.578947            0.553957   \n",
       "16           0.488889       ...                  0.578947            0.553957   \n",
       "17           0.488889       ...                  0.578947            0.553957   \n",
       "18           0.488889       ...                  0.578947            0.553957   \n",
       "19           0.488889       ...                  0.578947            0.553957   \n",
       "24           0.488889       ...                  0.578947            0.553957   \n",
       "25           0.488889       ...                  0.578947            0.553957   \n",
       "26           0.488889       ...                  0.578947            0.553957   \n",
       "32           0.488889       ...                  0.578947            0.553957   \n",
       "34           0.488889       ...                  0.578947            0.553957   \n",
       "27           0.488889       ...                  0.578947            0.553957   \n",
       "\n",
       "    split6_test_score  split6_train_score  split7_test_score  \\\n",
       "51           0.657895            0.910072           0.857143   \n",
       "5            0.657895            0.910072           0.857143   \n",
       "28           0.657895            0.910072           0.857143   \n",
       "55           0.631579            1.000000           0.800000   \n",
       "23           0.631579            1.000000           0.800000   \n",
       "22           0.631579            1.000000           0.800000   \n",
       "21           0.631579            1.000000           0.800000   \n",
       "54           0.631579            1.000000           0.800000   \n",
       "29           0.631579            1.000000           0.800000   \n",
       "36           0.631579            1.000000           0.800000   \n",
       "44           0.631579            1.000000           0.800000   \n",
       "15           0.631579            1.000000           0.800000   \n",
       "14           0.631579            1.000000           0.800000   \n",
       "45           0.631579            1.000000           0.800000   \n",
       "38           0.631579            1.000000           0.800000   \n",
       "46           0.631579            1.000000           0.800000   \n",
       "47           0.631579            1.000000           0.800000   \n",
       "30           0.631579            1.000000           0.800000   \n",
       "31           0.631579            1.000000           0.800000   \n",
       "7            0.631579            1.000000           0.800000   \n",
       "6            0.631579            1.000000           0.800000   \n",
       "37           0.631579            1.000000           0.800000   \n",
       "52           0.631579            1.000000           0.800000   \n",
       "53           0.631579            1.000000           0.800000   \n",
       "13           0.631579            1.000000           0.800000   \n",
       "39           0.631579            1.000000           0.800000   \n",
       "43           0.657895            0.694245           0.685714   \n",
       "20           0.657895            0.694245           0.685714   \n",
       "41           0.578947            0.553957           0.628571   \n",
       "42           0.578947            0.553957           0.628571   \n",
       "48           0.578947            0.553957           0.628571   \n",
       "49           0.578947            0.553957           0.628571   \n",
       "50           0.578947            0.553957           0.628571   \n",
       "40           0.578947            0.553957           0.628571   \n",
       "35           0.578947            0.553957           0.628571   \n",
       "0            0.578947            0.553957           0.628571   \n",
       "33           0.578947            0.553957           0.628571   \n",
       "1            0.578947            0.553957           0.628571   \n",
       "2            0.578947            0.553957           0.628571   \n",
       "3            0.578947            0.553957           0.628571   \n",
       "4            0.578947            0.553957           0.628571   \n",
       "8            0.578947            0.553957           0.628571   \n",
       "9            0.578947            0.553957           0.628571   \n",
       "10           0.578947            0.553957           0.628571   \n",
       "11           0.578947            0.553957           0.628571   \n",
       "12           0.578947            0.553957           0.628571   \n",
       "16           0.578947            0.553957           0.628571   \n",
       "17           0.578947            0.553957           0.628571   \n",
       "18           0.578947            0.553957           0.628571   \n",
       "19           0.578947            0.553957           0.628571   \n",
       "24           0.578947            0.553957           0.628571   \n",
       "25           0.578947            0.553957           0.628571   \n",
       "26           0.578947            0.553957           0.628571   \n",
       "32           0.578947            0.553957           0.628571   \n",
       "34           0.578947            0.553957           0.628571   \n",
       "27           0.578947            0.553957           0.628571   \n",
       "\n",
       "    split7_train_score  std_fit_time  std_score_time  std_test_score  \\\n",
       "51            0.900356      0.007564    6.502099e-07        0.053006   \n",
       "5             0.900356      0.010875    5.033594e-06        0.053006   \n",
       "28            0.900356      0.011687    5.173769e-03        0.053006   \n",
       "55            1.000000      0.007813    4.906081e-07        0.056433   \n",
       "23            1.000000      0.007815    4.607327e-07        0.056433   \n",
       "22            1.000000      0.007808    5.167483e-03        0.056433   \n",
       "21            1.000000      0.007813    5.230881e-06        0.056433   \n",
       "54            1.000000      0.014485    5.167934e-03        0.056433   \n",
       "29            1.000000      0.006992    8.311162e-06        0.056433   \n",
       "36            1.000000      0.010334    5.167393e-03        0.056433   \n",
       "44            1.000000      0.009199    5.862014e-04        0.056433   \n",
       "15            1.000000      0.007562    5.789250e-06        0.056433   \n",
       "14            1.000000      0.007811    2.384186e-07        0.056433   \n",
       "45            1.000000      0.011050    5.167280e-03        0.056433   \n",
       "38            1.000000      0.007565    7.812619e-03        0.056433   \n",
       "46            1.000000      0.007564    5.167404e-03        0.056433   \n",
       "47            1.000000      0.010875    1.014593e-06        0.056433   \n",
       "30            1.000000      0.007814    5.167292e-03        0.056433   \n",
       "31            1.000000      0.007562    6.765806e-03        0.056433   \n",
       "7             1.000000      0.010336    5.376904e-06        0.056433   \n",
       "6             1.000000      0.010870    7.881123e-06        0.056433   \n",
       "37            1.000000      0.007812    5.167449e-03        0.056433   \n",
       "52            1.000000      0.007564    5.167483e-03        0.056433   \n",
       "53            1.000000      0.007565    6.529362e-07        0.056433   \n",
       "13            1.000000      0.007565    2.779776e-07        0.056433   \n",
       "39            1.000000      0.007814    7.812560e-03        0.056433   \n",
       "43            0.690391      0.007564    6.765324e-03        0.048789   \n",
       "20            0.690391      0.010335    7.564297e-03        0.048789   \n",
       "41            0.548043      0.005168    7.564513e-03        0.038937   \n",
       "42            0.548043      0.005167    7.564667e-03        0.038937   \n",
       "48            0.548043      0.005167    6.765875e-03        0.038937   \n",
       "49            0.548043      0.005168    6.765462e-03        0.038937   \n",
       "50            0.548043      0.010334    6.071195e-07        0.038937   \n",
       "40            0.548043      0.005167    6.765841e-03        0.038937   \n",
       "35            0.548043      0.010875    7.348554e-07        0.038937   \n",
       "0             0.548043      0.005509    6.073389e-03        0.038937   \n",
       "33            0.548043      0.006765    7.564059e-03        0.038937   \n",
       "1             0.548043      0.005353    7.322045e-03        0.038937   \n",
       "2             0.548043      0.005261    6.095282e-03        0.038937   \n",
       "3             0.548043      0.006023    6.762211e-03        0.038937   \n",
       "4             0.548043      0.007956    7.974558e-07        0.038937   \n",
       "8             0.548043      0.006767    7.562737e-03        0.038937   \n",
       "9             0.548043      0.006766    7.561644e-03        0.038937   \n",
       "10            0.548043      0.005167    6.765806e-03        0.038937   \n",
       "11            0.548043      0.007813    7.812440e-03        0.038937   \n",
       "12            0.548043      0.006764    6.765617e-03        0.038937   \n",
       "16            0.548043      0.005167    2.308478e-07        0.038937   \n",
       "17            0.548043      0.005168    6.765497e-03        0.038937   \n",
       "18            0.548043      0.009526    7.564367e-03        0.038937   \n",
       "19            0.548043      0.005168    6.766944e-03        0.038937   \n",
       "24            0.548043      0.005163    5.166697e-03        0.038937   \n",
       "25            0.548043      0.005167    6.765944e-03        0.038937   \n",
       "26            0.548043      0.005157    7.810445e-03        0.038937   \n",
       "32            0.548043      0.005168    5.167404e-03        0.038937   \n",
       "34            0.548043      0.006761    3.153981e-07        0.038937   \n",
       "27            0.548043      0.009677    4.731989e-04        0.038937   \n",
       "\n",
       "    std_train_score  \n",
       "51         0.003325  \n",
       "5          0.003325  \n",
       "28         0.003325  \n",
       "55         0.000000  \n",
       "23         0.000000  \n",
       "22         0.000000  \n",
       "21         0.000000  \n",
       "54         0.000000  \n",
       "29         0.000000  \n",
       "36         0.000000  \n",
       "44         0.000000  \n",
       "15         0.000000  \n",
       "14         0.000000  \n",
       "45         0.000000  \n",
       "38         0.000000  \n",
       "46         0.000000  \n",
       "47         0.000000  \n",
       "30         0.000000  \n",
       "31         0.000000  \n",
       "7          0.000000  \n",
       "6          0.000000  \n",
       "37         0.000000  \n",
       "52         0.000000  \n",
       "53         0.000000  \n",
       "13         0.000000  \n",
       "39         0.000000  \n",
       "43         0.004366  \n",
       "20         0.004366  \n",
       "41         0.005633  \n",
       "42         0.005633  \n",
       "48         0.005633  \n",
       "49         0.005633  \n",
       "50         0.005633  \n",
       "40         0.005633  \n",
       "35         0.005633  \n",
       "0          0.005633  \n",
       "33         0.005633  \n",
       "1          0.005633  \n",
       "2          0.005633  \n",
       "3          0.005633  \n",
       "4          0.005633  \n",
       "8          0.005633  \n",
       "9          0.005633  \n",
       "10         0.005633  \n",
       "11         0.005633  \n",
       "12         0.005633  \n",
       "16         0.005633  \n",
       "17         0.005633  \n",
       "18         0.005633  \n",
       "19         0.005633  \n",
       "24         0.005633  \n",
       "25         0.005633  \n",
       "26         0.005633  \n",
       "32         0.005633  \n",
       "34         0.005633  \n",
       "27         0.005633  \n",
       "\n",
       "[56 rows x 29 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC_grid_results = pd.DataFrame(SVC_grid.cv_results_).sort_values('rank_test_score')\n",
    "SVC_grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for this Grid Search still overfits the training data. There is a 25% difference in mean train and test scores. As we did for LSVC, we will choose the model with the lowest variance: C = 10, gamma = 0.0001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'min_samples_leaf': [1, 5, 10, 20, 30, 40, 50], 'min_samples_split': [2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "DTF = DecisionTreeClassifier()\n",
    "\n",
    "params = {'min_samples_leaf': [1,5,10,20,30,40,50],\n",
    "          'min_samples_split': [2,3,4,5]}\n",
    "\n",
    "DTF_grid = GridSearchCV(estimator=DTF, param_grid=params, cv=kFold, n_jobs=-1)\n",
    "DTF_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 10, 'min_samples_split': 4}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.756458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.766187</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.744604</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.743772</td>\n",
       "      <td>0.010875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050914</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 10, 'min_samples_split': 3}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.756458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.766187</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.744604</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.743772</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 10, 'min_samples_split': 2}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.756458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.766187</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.744604</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.743772</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.677215</td>\n",
       "      <td>0.755445</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 10, 'min_samples_split': 5}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.756458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.766187</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.744604</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.743772</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048043</td>\n",
       "      <td>0.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.639241</td>\n",
       "      <td>0.830081</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 5, 'min_samples_split': 4}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.804270</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068697</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626582</td>\n",
       "      <td>0.830081</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 5, 'min_samples_split': 2}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.804270</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063667</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.626582</td>\n",
       "      <td>0.830081</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 5, 'min_samples_split': 3}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.804270</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065216</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.613924</td>\n",
       "      <td>0.830081</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 5, 'min_samples_split': 5}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.822878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.804270</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610759</td>\n",
       "      <td>0.679045</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 20, 'min_samples_split': 5}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.683274</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.009492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.610759</td>\n",
       "      <td>0.679045</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 20, 'min_samples_split': 2}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.683274</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041527</td>\n",
       "      <td>0.009492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.015626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.679045</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 20, 'min_samples_split': 4}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.683274</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.009492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.607595</td>\n",
       "      <td>0.679045</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 20, 'min_samples_split': 3}</td>\n",
       "      <td>11</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.683274</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.009492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.039063</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.929947</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 1, 'min_samples_split': 5}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.929889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.924460</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.925267</td>\n",
       "      <td>0.011049</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.035968</td>\n",
       "      <td>0.005086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.651487</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 30, 'min_samples_split': 5}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.654676</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.939461</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 1, 'min_samples_split': 4}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.948339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.946043</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.935252</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.932384</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.050073</td>\n",
       "      <td>0.006314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.651487</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 30, 'min_samples_split': 2}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.654676</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.651487</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 30, 'min_samples_split': 3}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.654676</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601266</td>\n",
       "      <td>0.651487</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 30, 'min_samples_split': 4}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.647482</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.654676</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.640569</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054955</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.035150</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.958869</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 1, 'min_samples_split': 3}</td>\n",
       "      <td>19</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.956835</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.953237</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.957295</td>\n",
       "      <td>0.010331</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.040885</td>\n",
       "      <td>0.005633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.026833</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.591772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 1, 'min_samples_split': 2}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009072</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.056093</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.585443</td>\n",
       "      <td>0.616574</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 40, 'min_samples_split': 2}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.616236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.615108</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.633452</td>\n",
       "      <td>0.007813</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.585443</td>\n",
       "      <td>0.616574</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 40, 'min_samples_split': 3}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.616236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.615108</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.633452</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585443</td>\n",
       "      <td>0.616574</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 40, 'min_samples_split': 4}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.616236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.615108</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.633452</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.585443</td>\n",
       "      <td>0.616574</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 40, 'min_samples_split': 5}</td>\n",
       "      <td>21</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.616236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.615108</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.633452</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055988</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.597620</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_samples_leaf': 50, 'min_samples_split': 2}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.594096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.013154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.597620</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_samples_leaf': 50, 'min_samples_split': 3}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.594096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.013154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.597620</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_samples_leaf': 50, 'min_samples_split': 4}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.594096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.013154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.582278</td>\n",
       "      <td>0.597620</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_samples_leaf': 50, 'min_samples_split': 5}</td>\n",
       "      <td>25</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.594096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.597865</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063207</td>\n",
       "      <td>0.013154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "10       0.025391         0.000000         0.680380          0.755445   \n",
       "9        0.021485         0.000000         0.677215          0.755445   \n",
       "8        0.023438         0.000000         0.677215          0.755445   \n",
       "11       0.019531         0.000000         0.677215          0.755445   \n",
       "6        0.027344         0.000000         0.639241          0.830081   \n",
       "4        0.027344         0.000000         0.626582          0.830081   \n",
       "5        0.029297         0.000000         0.626582          0.830081   \n",
       "7        0.023438         0.001953         0.613924          0.830081   \n",
       "15       0.015625         0.000000         0.610759          0.679045   \n",
       "12       0.015625         0.000000         0.610759          0.679045   \n",
       "14       0.015626         0.000000         0.607595          0.679045   \n",
       "13       0.015625         0.000000         0.607595          0.679045   \n",
       "3        0.039063         0.001953         0.601266          0.929947   \n",
       "19       0.009766         0.000000         0.601266          0.651487   \n",
       "2        0.037109         0.001953         0.601266          0.939461   \n",
       "16       0.013672         0.000000         0.601266          0.651487   \n",
       "17       0.011719         0.000000         0.601266          0.651487   \n",
       "18       0.013672         0.000000         0.601266          0.651487   \n",
       "1        0.035150         0.001953         0.594937          0.958869   \n",
       "0        0.026833         0.000125         0.591772          1.000000   \n",
       "20       0.007812         0.001953         0.585443          0.616574   \n",
       "21       0.009766         0.001953         0.585443          0.616574   \n",
       "22       0.009766         0.000000         0.585443          0.616574   \n",
       "23       0.009766         0.000000         0.585443          0.616574   \n",
       "24       0.009766         0.000000         0.582278          0.597620   \n",
       "25       0.005859         0.000000         0.582278          0.597620   \n",
       "26       0.005859         0.000000         0.582278          0.597620   \n",
       "27       0.005859         0.000000         0.582278          0.597620   \n",
       "\n",
       "   param_min_samples_leaf param_min_samples_split  \\\n",
       "10                     10                       4   \n",
       "9                      10                       3   \n",
       "8                      10                       2   \n",
       "11                     10                       5   \n",
       "6                       5                       4   \n",
       "4                       5                       2   \n",
       "5                       5                       3   \n",
       "7                       5                       5   \n",
       "15                     20                       5   \n",
       "12                     20                       2   \n",
       "14                     20                       4   \n",
       "13                     20                       3   \n",
       "3                       1                       5   \n",
       "19                     30                       5   \n",
       "2                       1                       4   \n",
       "16                     30                       2   \n",
       "17                     30                       3   \n",
       "18                     30                       4   \n",
       "1                       1                       3   \n",
       "0                       1                       2   \n",
       "20                     40                       2   \n",
       "21                     40                       3   \n",
       "22                     40                       4   \n",
       "23                     40                       5   \n",
       "24                     50                       2   \n",
       "25                     50                       3   \n",
       "26                     50                       4   \n",
       "27                     50                       5   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "10  {'min_samples_leaf': 10, 'min_samples_split': 4}                1   \n",
       "9   {'min_samples_leaf': 10, 'min_samples_split': 3}                2   \n",
       "8   {'min_samples_leaf': 10, 'min_samples_split': 2}                2   \n",
       "11  {'min_samples_leaf': 10, 'min_samples_split': 5}                2   \n",
       "6    {'min_samples_leaf': 5, 'min_samples_split': 4}                5   \n",
       "4    {'min_samples_leaf': 5, 'min_samples_split': 2}                6   \n",
       "5    {'min_samples_leaf': 5, 'min_samples_split': 3}                6   \n",
       "7    {'min_samples_leaf': 5, 'min_samples_split': 5}                8   \n",
       "15  {'min_samples_leaf': 20, 'min_samples_split': 5}                9   \n",
       "12  {'min_samples_leaf': 20, 'min_samples_split': 2}                9   \n",
       "14  {'min_samples_leaf': 20, 'min_samples_split': 4}               11   \n",
       "13  {'min_samples_leaf': 20, 'min_samples_split': 3}               11   \n",
       "3    {'min_samples_leaf': 1, 'min_samples_split': 5}               13   \n",
       "19  {'min_samples_leaf': 30, 'min_samples_split': 5}               13   \n",
       "2    {'min_samples_leaf': 1, 'min_samples_split': 4}               13   \n",
       "16  {'min_samples_leaf': 30, 'min_samples_split': 2}               13   \n",
       "17  {'min_samples_leaf': 30, 'min_samples_split': 3}               13   \n",
       "18  {'min_samples_leaf': 30, 'min_samples_split': 4}               13   \n",
       "1    {'min_samples_leaf': 1, 'min_samples_split': 3}               19   \n",
       "0    {'min_samples_leaf': 1, 'min_samples_split': 2}               20   \n",
       "20  {'min_samples_leaf': 40, 'min_samples_split': 2}               21   \n",
       "21  {'min_samples_leaf': 40, 'min_samples_split': 3}               21   \n",
       "22  {'min_samples_leaf': 40, 'min_samples_split': 4}               21   \n",
       "23  {'min_samples_leaf': 40, 'min_samples_split': 5}               21   \n",
       "24  {'min_samples_leaf': 50, 'min_samples_split': 2}               25   \n",
       "25  {'min_samples_leaf': 50, 'min_samples_split': 3}               25   \n",
       "26  {'min_samples_leaf': 50, 'min_samples_split': 4}               25   \n",
       "27  {'min_samples_leaf': 50, 'min_samples_split': 5}               25   \n",
       "\n",
       "    split0_test_score  split0_train_score       ...         split5_test_score  \\\n",
       "10           0.622222            0.756458       ...                  0.736842   \n",
       "9            0.622222            0.756458       ...                  0.710526   \n",
       "8            0.622222            0.756458       ...                  0.710526   \n",
       "11           0.622222            0.756458       ...                  0.710526   \n",
       "6            0.666667            0.822878       ...                  0.552632   \n",
       "4            0.666667            0.822878       ...                  0.526316   \n",
       "5            0.666667            0.822878       ...                  0.578947   \n",
       "7            0.644444            0.822878       ...                  0.552632   \n",
       "15           0.577778            0.686347       ...                  0.631579   \n",
       "12           0.577778            0.686347       ...                  0.631579   \n",
       "14           0.577778            0.686347       ...                  0.631579   \n",
       "13           0.577778            0.686347       ...                  0.631579   \n",
       "3            0.622222            0.929889       ...                  0.526316   \n",
       "19           0.511111            0.660517       ...                  0.657895   \n",
       "2            0.666667            0.948339       ...                  0.500000   \n",
       "16           0.511111            0.660517       ...                  0.657895   \n",
       "17           0.511111            0.660517       ...                  0.657895   \n",
       "18           0.511111            0.660517       ...                  0.657895   \n",
       "1            0.644444            0.963100       ...                  0.500000   \n",
       "0            0.688889            1.000000       ...                  0.500000   \n",
       "20           0.511111            0.616236       ...                  0.684211   \n",
       "21           0.511111            0.616236       ...                  0.684211   \n",
       "22           0.511111            0.616236       ...                  0.684211   \n",
       "23           0.511111            0.616236       ...                  0.684211   \n",
       "24           0.511111            0.594096       ...                  0.684211   \n",
       "25           0.511111            0.594096       ...                  0.684211   \n",
       "26           0.511111            0.594096       ...                  0.684211   \n",
       "27           0.511111            0.594096       ...                  0.684211   \n",
       "\n",
       "    split5_train_score  split6_test_score  split6_train_score  \\\n",
       "10            0.766187           0.657895            0.744604   \n",
       "9             0.766187           0.657895            0.744604   \n",
       "8             0.766187           0.657895            0.744604   \n",
       "11            0.766187           0.657895            0.744604   \n",
       "6             0.848921           0.684211            0.802158   \n",
       "4             0.848921           0.684211            0.802158   \n",
       "5             0.848921           0.605263            0.802158   \n",
       "7             0.848921           0.578947            0.802158   \n",
       "15            0.676259           0.552632            0.676259   \n",
       "12            0.676259           0.552632            0.676259   \n",
       "14            0.676259           0.552632            0.676259   \n",
       "13            0.676259           0.552632            0.676259   \n",
       "3             0.928058           0.578947            0.924460   \n",
       "19            0.647482           0.605263            0.654676   \n",
       "2             0.946043           0.605263            0.935252   \n",
       "16            0.647482           0.605263            0.654676   \n",
       "17            0.647482           0.605263            0.654676   \n",
       "18            0.647482           0.605263            0.654676   \n",
       "1             0.956835           0.578947            0.953237   \n",
       "0             1.000000           0.605263            1.000000   \n",
       "20            0.615108           0.552632            0.633094   \n",
       "21            0.615108           0.552632            0.633094   \n",
       "22            0.615108           0.552632            0.633094   \n",
       "23            0.615108           0.552632            0.633094   \n",
       "24            0.604317           0.526316            0.618705   \n",
       "25            0.604317           0.526316            0.618705   \n",
       "26            0.604317           0.526316            0.618705   \n",
       "27            0.604317           0.526316            0.618705   \n",
       "\n",
       "    split7_test_score  split7_train_score  std_fit_time  std_score_time  \\\n",
       "10           0.685714            0.743772      0.010875        0.000000   \n",
       "9            0.685714            0.743772      0.007565        0.000000   \n",
       "8            0.685714            0.743772      0.007813        0.000000   \n",
       "11           0.685714            0.743772      0.006766        0.000000   \n",
       "6            0.571429            0.804270      0.006766        0.000000   \n",
       "4            0.571429            0.804270      0.006766        0.000000   \n",
       "5            0.571429            0.804270      0.005168        0.000000   \n",
       "7            0.571429            0.804270      0.007813        0.005167   \n",
       "15           0.628571            0.683274      0.000002        0.000000   \n",
       "12           0.628571            0.683274      0.000002        0.000000   \n",
       "14           0.628571            0.683274      0.007813        0.000000   \n",
       "13           0.628571            0.683274      0.007813        0.000000   \n",
       "3            0.657143            0.925267      0.011049        0.005168   \n",
       "19           0.685714            0.640569      0.007565        0.000000   \n",
       "2            0.628571            0.932384      0.007565        0.005168   \n",
       "16           0.685714            0.640569      0.005167        0.000000   \n",
       "17           0.685714            0.640569      0.006766        0.000000   \n",
       "18           0.685714            0.640569      0.005168        0.000000   \n",
       "1            0.600000            0.957295      0.010331        0.005168   \n",
       "0            0.600000            1.000000      0.009072        0.000331   \n",
       "20           0.600000            0.633452      0.007813        0.005168   \n",
       "21           0.600000            0.633452      0.007565        0.005168   \n",
       "22           0.600000            0.633452      0.007565        0.000000   \n",
       "23           0.600000            0.633452      0.007565        0.000000   \n",
       "24           0.628571            0.597865      0.007565        0.000000   \n",
       "25           0.628571            0.597865      0.007565        0.000000   \n",
       "26           0.628571            0.597865      0.007564        0.000000   \n",
       "27           0.628571            0.597865      0.007564        0.000000   \n",
       "\n",
       "    std_test_score  std_train_score  \n",
       "10        0.050914         0.007831  \n",
       "9         0.048043         0.007831  \n",
       "8         0.048043         0.007831  \n",
       "11        0.048043         0.007831  \n",
       "6         0.068697         0.018033  \n",
       "4         0.063667         0.018033  \n",
       "5         0.065216         0.018033  \n",
       "7         0.064241         0.018033  \n",
       "15        0.041527         0.009492  \n",
       "12        0.041527         0.009492  \n",
       "14        0.044186         0.009492  \n",
       "13        0.044186         0.009492  \n",
       "3         0.035968         0.005086  \n",
       "19        0.054955         0.005850  \n",
       "2         0.050073         0.006314  \n",
       "16        0.054955         0.005850  \n",
       "17        0.054955         0.005850  \n",
       "18        0.054955         0.005850  \n",
       "1         0.040885         0.005633  \n",
       "0         0.056093         0.000000  \n",
       "20        0.055988         0.012672  \n",
       "21        0.055988         0.012672  \n",
       "22        0.055988         0.012672  \n",
       "23        0.055988         0.012672  \n",
       "24        0.063207         0.013154  \n",
       "25        0.063207         0.013154  \n",
       "26        0.063207         0.013154  \n",
       "27        0.063207         0.013154  \n",
       "\n",
       "[28 rows x 28 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DTF_grid_results = pd.DataFrame(DTF_grid.cv_results_).sort_values('rank_test_score')\n",
    "DTF_grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best possible model also has high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=8, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [80, 100, 140, 200], 'max_depth': [1, 2, 3, 4, 5]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=300, criterion='gini')\n",
    "\n",
    "params = {'max_features': [80,100,140,200],\n",
    "          'max_depth': [1,2,3,4,5]}\n",
    "\n",
    "RF_grid = GridSearchCV(estimator=RF, param_grid=params, cv=kFold, n_jobs=-1)\n",
    "RF_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split5_test_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_test_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_test_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.117340</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.724684</td>\n",
       "      <td>0.879783</td>\n",
       "      <td>5</td>\n",
       "      <td>140</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 140}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.861210</td>\n",
       "      <td>0.023306</td>\n",
       "      <td>7.812649e-03</td>\n",
       "      <td>0.051846</td>\n",
       "      <td>0.009074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.660532</td>\n",
       "      <td>0.020869</td>\n",
       "      <td>0.715190</td>\n",
       "      <td>0.882030</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 200}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.881919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.895683</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.892086</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.861210</td>\n",
       "      <td>0.096080</td>\n",
       "      <td>8.169024e-03</td>\n",
       "      <td>0.054028</td>\n",
       "      <td>0.009936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.681319</td>\n",
       "      <td>0.026269</td>\n",
       "      <td>0.712025</td>\n",
       "      <td>0.877517</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 100}</td>\n",
       "      <td>3</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.881295</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.888489</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.857651</td>\n",
       "      <td>0.018037</td>\n",
       "      <td>1.640723e-03</td>\n",
       "      <td>0.050408</td>\n",
       "      <td>0.011037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.452136</td>\n",
       "      <td>0.026894</td>\n",
       "      <td>0.699367</td>\n",
       "      <td>0.866659</td>\n",
       "      <td>5</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_depth': 5, 'max_features': 80}</td>\n",
       "      <td>4</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.885609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.870504</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.868327</td>\n",
       "      <td>0.016615</td>\n",
       "      <td>1.764467e-03</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>0.010178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.370657</td>\n",
       "      <td>0.026923</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.839099</td>\n",
       "      <td>4</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 200}</td>\n",
       "      <td>5</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.845018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.838129</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.845324</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.822064</td>\n",
       "      <td>0.030648</td>\n",
       "      <td>2.146300e-03</td>\n",
       "      <td>0.050521</td>\n",
       "      <td>0.009238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.451689</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.686709</td>\n",
       "      <td>0.811532</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 100}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.822878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.797153</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>5.646734e-07</td>\n",
       "      <td>0.054520</td>\n",
       "      <td>0.011204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.254140</td>\n",
       "      <td>0.026644</td>\n",
       "      <td>0.686709</td>\n",
       "      <td>0.803871</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 80}</td>\n",
       "      <td>6</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.815498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.791367</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.802158</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.786477</td>\n",
       "      <td>0.024674</td>\n",
       "      <td>1.495908e-03</td>\n",
       "      <td>0.044249</td>\n",
       "      <td>0.010156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.826196</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.683544</td>\n",
       "      <td>0.828238</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>{'max_depth': 4, 'max_features': 140}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.837638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.827338</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.830935</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.818505</td>\n",
       "      <td>0.019822</td>\n",
       "      <td>7.812470e-03</td>\n",
       "      <td>0.034975</td>\n",
       "      <td>0.007916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.423636</td>\n",
       "      <td>0.025018</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.741471</td>\n",
       "      <td>3</td>\n",
       "      <td>140</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 140}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.752768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.733813</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.729537</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>1.659558e-03</td>\n",
       "      <td>0.046850</td>\n",
       "      <td>0.008945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.198917</td>\n",
       "      <td>0.024892</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.733306</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 100}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.738007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.744604</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.711744</td>\n",
       "      <td>0.013665</td>\n",
       "      <td>1.054116e-03</td>\n",
       "      <td>0.047948</td>\n",
       "      <td>0.011226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.022668</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.664557</td>\n",
       "      <td>0.732885</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 80}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.745387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.712230</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.751799</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>0.012462</td>\n",
       "      <td>7.564890e-03</td>\n",
       "      <td>0.048595</td>\n",
       "      <td>0.012985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.818041</td>\n",
       "      <td>0.024517</td>\n",
       "      <td>0.658228</td>\n",
       "      <td>0.743328</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 200}</td>\n",
       "      <td>12</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.774908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.730216</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.722420</td>\n",
       "      <td>0.026390</td>\n",
       "      <td>1.323602e-03</td>\n",
       "      <td>0.051908</td>\n",
       "      <td>0.016562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.102390</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.655063</td>\n",
       "      <td>0.682250</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 140}</td>\n",
       "      <td>13</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.679856</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>6.766133e-03</td>\n",
       "      <td>0.044487</td>\n",
       "      <td>0.010896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.360676</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.648734</td>\n",
       "      <td>0.680425</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 200}</td>\n",
       "      <td>14</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.665468</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.690647</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.017797</td>\n",
       "      <td>7.564359e-03</td>\n",
       "      <td>0.049347</td>\n",
       "      <td>0.008435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.642405</td>\n",
       "      <td>0.677236</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 100}</td>\n",
       "      <td>15</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.675277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.669065</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.676259</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.011532</td>\n",
       "      <td>7.564344e-03</td>\n",
       "      <td>0.047740</td>\n",
       "      <td>0.008560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.841809</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.639241</td>\n",
       "      <td>0.675005</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 80}</td>\n",
       "      <td>16</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.686347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.665468</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.665468</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.672598</td>\n",
       "      <td>0.012198</td>\n",
       "      <td>7.812768e-03</td>\n",
       "      <td>0.035544</td>\n",
       "      <td>0.009141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.912793</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.569620</td>\n",
       "      <td>0.583678</td>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 200}</td>\n",
       "      <td>17</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.579336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.575540</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.622302</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.565836</td>\n",
       "      <td>0.011407</td>\n",
       "      <td>6.766133e-03</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.020198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.697270</td>\n",
       "      <td>0.029298</td>\n",
       "      <td>0.560127</td>\n",
       "      <td>0.570134</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 100}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.575646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.564748</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.600719</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>5.167607e-03</td>\n",
       "      <td>0.041576</td>\n",
       "      <td>0.015601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.406320</td>\n",
       "      <td>0.010660</td>\n",
       "      <td>0.560127</td>\n",
       "      <td>0.569635</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 80}</td>\n",
       "      <td>18</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.571956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.568345</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.604317</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.558719</td>\n",
       "      <td>0.100660</td>\n",
       "      <td>5.824550e-03</td>\n",
       "      <td>0.045274</td>\n",
       "      <td>0.015621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.789365</td>\n",
       "      <td>0.021485</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.579181</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 140}</td>\n",
       "      <td>20</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.601476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.553957</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.618705</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.576512</td>\n",
       "      <td>0.010868</td>\n",
       "      <td>7.564259e-03</td>\n",
       "      <td>0.032156</td>\n",
       "      <td>0.021250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "18       2.117340         0.023438         0.724684          0.879783   \n",
       "19       2.660532         0.020869         0.715190          0.882030   \n",
       "17       1.681319         0.026269         0.712025          0.877517   \n",
       "16       1.452136         0.026894         0.699367          0.866659   \n",
       "15       2.370657         0.026923         0.696203          0.839099   \n",
       "13       1.451689         0.031251         0.686709          0.811532   \n",
       "12       1.254140         0.026644         0.686709          0.803871   \n",
       "14       1.826196         0.023438         0.683544          0.828238   \n",
       "10       1.423636         0.025018         0.664557          0.741471   \n",
       "9        1.198917         0.024892         0.664557          0.733306   \n",
       "8        1.022668         0.021485         0.664557          0.732885   \n",
       "11       1.818041         0.024517         0.658228          0.743328   \n",
       "6        1.102390         0.027344         0.655063          0.682250   \n",
       "7        1.360676         0.021485         0.648734          0.680425   \n",
       "5        0.926908         0.025391         0.642405          0.677236   \n",
       "4        0.841809         0.023438         0.639241          0.675005   \n",
       "3        0.912793         0.027344         0.569620          0.583678   \n",
       "1        0.697270         0.029298         0.560127          0.570134   \n",
       "0        0.406320         0.010660         0.560127          0.569635   \n",
       "2        0.789365         0.021485         0.556962          0.579181   \n",
       "\n",
       "   param_max_depth param_max_features                                 params  \\\n",
       "18               5                140  {'max_depth': 5, 'max_features': 140}   \n",
       "19               5                200  {'max_depth': 5, 'max_features': 200}   \n",
       "17               5                100  {'max_depth': 5, 'max_features': 100}   \n",
       "16               5                 80   {'max_depth': 5, 'max_features': 80}   \n",
       "15               4                200  {'max_depth': 4, 'max_features': 200}   \n",
       "13               4                100  {'max_depth': 4, 'max_features': 100}   \n",
       "12               4                 80   {'max_depth': 4, 'max_features': 80}   \n",
       "14               4                140  {'max_depth': 4, 'max_features': 140}   \n",
       "10               3                140  {'max_depth': 3, 'max_features': 140}   \n",
       "9                3                100  {'max_depth': 3, 'max_features': 100}   \n",
       "8                3                 80   {'max_depth': 3, 'max_features': 80}   \n",
       "11               3                200  {'max_depth': 3, 'max_features': 200}   \n",
       "6                2                140  {'max_depth': 2, 'max_features': 140}   \n",
       "7                2                200  {'max_depth': 2, 'max_features': 200}   \n",
       "5                2                100  {'max_depth': 2, 'max_features': 100}   \n",
       "4                2                 80   {'max_depth': 2, 'max_features': 80}   \n",
       "3                1                200  {'max_depth': 1, 'max_features': 200}   \n",
       "1                1                100  {'max_depth': 1, 'max_features': 100}   \n",
       "0                1                 80   {'max_depth': 1, 'max_features': 80}   \n",
       "2                1                140  {'max_depth': 1, 'max_features': 140}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score       ...         \\\n",
       "18                1           0.622222            0.885609       ...          \n",
       "19                2           0.622222            0.881919       ...          \n",
       "17                3           0.622222            0.885609       ...          \n",
       "16                4           0.622222            0.885609       ...          \n",
       "15                5           0.622222            0.845018       ...          \n",
       "13                6           0.600000            0.822878       ...          \n",
       "12                6           0.600000            0.815498       ...          \n",
       "14                8           0.622222            0.837638       ...          \n",
       "10                9           0.600000            0.752768       ...          \n",
       "9                 9           0.600000            0.738007       ...          \n",
       "8                 9           0.600000            0.745387       ...          \n",
       "11               12           0.577778            0.774908       ...          \n",
       "6                13           0.600000            0.686347       ...          \n",
       "7                14           0.577778            0.686347       ...          \n",
       "5                15           0.577778            0.675277       ...          \n",
       "4                16           0.600000            0.686347       ...          \n",
       "3                17           0.488889            0.579336       ...          \n",
       "1                18           0.488889            0.575646       ...          \n",
       "0                18           0.488889            0.571956       ...          \n",
       "2                20           0.511111            0.601476       ...          \n",
       "\n",
       "    split5_test_score  split5_train_score  split6_test_score  \\\n",
       "18           0.763158            0.888489           0.736842   \n",
       "19           0.763158            0.895683           0.710526   \n",
       "17           0.736842            0.881295           0.763158   \n",
       "16           0.736842            0.870504           0.710526   \n",
       "15           0.736842            0.838129           0.684211   \n",
       "13           0.710526            0.805755           0.684211   \n",
       "12           0.684211            0.791367           0.710526   \n",
       "14           0.684211            0.827338           0.710526   \n",
       "10           0.684211            0.733813           0.684211   \n",
       "9            0.684211            0.741007           0.657895   \n",
       "8            0.684211            0.712230           0.657895   \n",
       "11           0.684211            0.741007           0.684211   \n",
       "6            0.657895            0.679856           0.657895   \n",
       "7            0.657895            0.665468           0.657895   \n",
       "5            0.657895            0.669065           0.657895   \n",
       "4            0.657895            0.665468           0.657895   \n",
       "3            0.631579            0.575540           0.578947   \n",
       "1            0.578947            0.564748           0.605263   \n",
       "0            0.578947            0.568345           0.578947   \n",
       "2            0.578947            0.553957           0.578947   \n",
       "\n",
       "    split6_train_score  split7_test_score  split7_train_score  std_fit_time  \\\n",
       "18            0.888489           0.742857            0.861210      0.023306   \n",
       "19            0.892086           0.714286            0.861210      0.096080   \n",
       "17            0.888489           0.742857            0.857651      0.018037   \n",
       "16            0.877698           0.771429            0.868327      0.016615   \n",
       "15            0.845324           0.685714            0.822064      0.030648   \n",
       "13            0.802158           0.742857            0.797153      0.012009   \n",
       "12            0.802158           0.685714            0.786477      0.024674   \n",
       "14            0.830935           0.714286            0.818505      0.019822   \n",
       "10            0.741007           0.685714            0.729537      0.016534   \n",
       "9             0.744604           0.685714            0.711744      0.013665   \n",
       "8             0.751799           0.685714            0.718861      0.012462   \n",
       "11            0.730216           0.657143            0.722420      0.026390   \n",
       "6             0.679856           0.657143            0.672598      0.006437   \n",
       "7             0.690647           0.628571            0.672598      0.017797   \n",
       "5             0.676259           0.628571            0.672598      0.011532   \n",
       "4             0.665468           0.657143            0.672598      0.012198   \n",
       "3             0.622302           0.628571            0.565836      0.011407   \n",
       "1             0.600719           0.628571            0.548043      0.007565   \n",
       "0             0.604317           0.657143            0.558719      0.100660   \n",
       "2             0.618705           0.600000            0.576512      0.010868   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "18    7.812649e-03        0.051846         0.009074  \n",
       "19    8.169024e-03        0.054028         0.009936  \n",
       "17    1.640723e-03        0.050408         0.011037  \n",
       "16    1.764467e-03        0.049470         0.010178  \n",
       "15    2.146300e-03        0.050521         0.009238  \n",
       "13    5.646734e-07        0.054520         0.011204  \n",
       "12    1.495908e-03        0.044249         0.010156  \n",
       "14    7.812470e-03        0.034975         0.007916  \n",
       "10    1.659558e-03        0.046850         0.008945  \n",
       "9     1.054116e-03        0.047948         0.011226  \n",
       "8     7.564890e-03        0.048595         0.012985  \n",
       "11    1.323602e-03        0.051908         0.016562  \n",
       "6     6.766133e-03        0.044487         0.010896  \n",
       "7     7.564359e-03        0.049347         0.008435  \n",
       "5     7.564344e-03        0.047740         0.008560  \n",
       "4     7.812768e-03        0.035544         0.009141  \n",
       "3     6.766133e-03        0.047532         0.020198  \n",
       "1     5.167607e-03        0.041576         0.015601  \n",
       "0     5.824550e-03        0.045274         0.015621  \n",
       "2     7.564259e-03        0.032156         0.021250  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_grid_results = pd.DataFrame(RF_grid.cv_results_).sort_values('rank_test_score')\n",
    "RF_grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is overfitting due to the large difference in mean train and test scores. The best model is {'max_depth': 2, 'max_features': 140}\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier with LinearSVC, Random Forest, and KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "vote_clf = VotingClassifier(estimators=[('LinearSVC', LinearSVC(multi_class='crammer_singer', C=0.01)), \n",
    "                                        ('RandomF', RandomForestClassifier(n_estimators=300, criterion='gini', max_depth=2, max_features=200)),\n",
    "                                        ('KNN', KNeighborsClassifier(n_neighbors=5))])\n",
    "\n",
    "accuracies = cross_val_score(estimator=vote_clf,X=X_train,y=y_train, scoring='accuracy', cv=kFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.5719251762536423\n",
      "Std.Dev of Accuracy: 0.03445185327570653\n",
      "Test set Accuracy: 0.5661764705882353\n"
     ]
    }
   ],
   "source": [
    "vote_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Mean Accuracy: {}'.format(accuracies.mean()))\n",
    "print('Std.Dev of Accuracy: {}'.format(accuracies.std()))\n",
    "print('Test set Accuracy: {}'.format(vote_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models with Bagging and/or Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is highly dimensional (has many features) and contains only a few observations, we need to use bootstrap aggregating (aka bagging). This means random subsets of the observations and features are drawn with replacement, and a model is trained on each subset. Sklearn's BaggingClassifier function will do the job seamlessly, then will use an aggregate function to make the most reliable predicitons. This method will reduce the variance of the predictions, which are not only due to the high dimensionality and small number of observations, but also due to the large imbalance in the target classes. However, bagging does NOT reduce the bias of the models; we do not expect to see an improvement in accuracy. This is where boosting comes in; the models are trained sequentially, taking into account underfitted datapoints for the next model. For the bagging models, we will use the hyperparameters that lead to high variance and low bias. As for the boosting models, we will use the hyperparameters that lead to low variance and high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use all records for bagging/boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaleX = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaleX.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn, n_estimators=300,bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=bag_knn, X=X_scaled, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.549040394251\n",
      "Std.Dev of accuracy: 0.0348924265577\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "kFold = StratifiedKFold(n_splits = 8)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "paste_knn = BaggingClassifier(base_estimator=knn, bootstrap=False, n_estimators=300,bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=paste_knn, X=X_scaled, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.546991213923\n",
      "Std.Dev of accuracy: 0.0367026058087\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that bagging/pasting reduced the variance but not that bias as expected. This is still not a good model, but at least it generalizes well. We will now use two boosting methods in order to lower the bias. KNeighborsClassifier does not support sample weights, so we will not be able to use Adaptive Boosting to lower the model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "kFold = StratifiedKFold(n_splits = 8)\n",
    "\n",
    "LR = LogisticRegression(C=1, multi_class='multinomial', solver='lbfgs')\n",
    "LR_bag = BaggingClassifier(base_estimator=LR, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LR_bag, X=X_scaled, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.544587367769\n",
      "Std.Dev of accuracy: 0.0339289227454\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm decreased the variance successfully, but raised the bias. Perhaps adaptive boosting could help raise it back to par."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "kFold = StratifiedKFold(n_splits = 8)\n",
    "\n",
    "LR = LogisticRegression(C=1, multi_class='multinomial', solver='lbfgs')\n",
    "LR_paste = BaggingClassifier(base_estimator=LR, n_estimators=300, bootstrap=False, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LR_paste, X=X_scaled, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.546991213923\n",
      "Std.Dev of accuracy: 0.0367026058087\n"
     ]
    }
   ],
   "source": [
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pasting algorithm decreased the variance more than bagging, but raised the bias. Perhaps adaptive boosting could help raise it back to par."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.645569620253\n",
      "Test Accuracy:  0.580882352941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "LR2 = LogisticRegression(C=0.1, multi_class='multinomial', solver='lbfgs')\n",
    "LR_ada = AdaBoostClassifier(base_estimator=LR2, algorithm='SAMME',n_estimators=300, learning_rate=0.5)\n",
    "LR_ada.fit(X_train, y_train)\n",
    "print('Train Accuracy: ', LR_ada.score(X_train, y_train))\n",
    "print('Test Accuracy: ', LR_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting lowered the bias, but the variance is still high. A good approach would be to combine both bagging and boosting to logistic regression. However, this is not optimal seeing that the bias could still be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LSVC = LinearSVC(C=1, multi_class='crammer_singer')\n",
    "LSVC_bag = BaggingClassifier(base_estimator=LSVC, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.715197261471\n",
      "Std.Dev of accuracy: 0.0268268912503\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=LSVC_bag, X=X_scaled, y=y, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the variance has been improved, however the bias is still high so will try adaptive boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LSVC = LinearSVC(C=1, multi_class='crammer_singer')\n",
    "LSVC_paste = BaggingClassifier(base_estimator=LSVC, n_estimators=300, bootstrap=False,bootstrap_features=True, max_samples=100, max_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy: 0.716971038621\n",
      "Std.Dev of accuracy: 0.0513783713695\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=LSVC_paste, X=X_scaled, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)\n",
    "print('Mean accuracy:', accuracies.mean())\n",
    "print('Std.Dev of accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.613924050633\n",
      "Test Accuracy:  0.551470588235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC2 = LinearSVC(C=0.01, multi_class='crammer_singer')\n",
    "\n",
    "LSVC_ada = AdaBoostClassifier(base_estimator=LSVC2, algorithm='SAMME',n_estimators=300, learning_rate=0.5)\n",
    "LSVC_ada.fit(X_train, y_train)\n",
    "print('Train Accuracy: ', LSVC_ada.score(X_train, y_train))\n",
    "print('Test Accuracy: ', LSVC_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the average accuracy for the LinearSVC. However, it will not work on its own; we need to do both bagging and boosting for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SVC = SVC(C=0.0001, gamma=1, kernel='poly')\n",
    "SVC_OVR = OneVsRestClassifier(estimator=SVC)\n",
    "SVC_bag = BaggingClassifier(base_estimator=SVC, n_estimators=100, bootstrap_features=True, max_samples=200, max_features=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.662822904146\n",
      "Std.Dev of Accuracy:  0.0575169179181\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=SVC_bag, X=X_scaled, y=y, scoring='accuracy', cv=5)\n",
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SVC = SVC(C=0.0001, gamma=1, kernel='poly')\n",
    "SVC_OVR = OneVsRestClassifier(estimator=SVC)\n",
    "SVC_bag = BaggingClassifier(base_estimator=SVC, n_estimators=100, bootstrap=False,bootstrap_features=True, max_samples=200, max_features=X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.693487839795\n",
      "Std.Dev of Accuracy:  0.0581710775429\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator=SVC_bag, X=X_scaled, y=y, scoring='accuracy', cv=5)\n",
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the variance of the model has improved with bagging. We cannot apply adaptive boosting to increase the accuracy of the model since OneVsRestClassifier does not support sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DTF = DecisionTreeClassifier(min_samples_leaf=10, min_impurity_split=3)\n",
    "DTF_bag = BaggingClassifier(base_estimator=DTF, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=DTF_bag, X=X_scaled, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DTF = DecisionTreeClassifier(min_samples_leaf=10, min_impurity_split=3)\n",
    "DTF_paste = BaggingClassifier(base_estimator=DTF, n_estimators=300, bootstrap=False, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=DTF_paste, X=X_scaled, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=2, min_impurity_split=None,\n",
       "            min_samples_leaf=50, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "          learning_rate=0.5, n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "DTF2 = DecisionTreeClassifier(min_samples_leaf=50, min_impurity_decrease=2)\n",
    "DTF_ada = AdaBoostClassifier(base_estimator=DTF2, algorithm='SAMME',n_estimators=300, learning_rate=0.5)\n",
    "DTF_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.556962025316\n",
      "Test Accuracy:  0.507352941176\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', DTF_ada.score(X_train, y_train))\n",
    "print('Test Accuracy: ', DTF_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=300, max_features=140, max_depth=5)\n",
    "RF_bag = BaggingClassifier(base_estimator=RF, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=RF_bag, X=X_scaled, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.626595060915\n",
      "Std.Dev of Accuracy:  0.0334773080778\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "RF2 = RandomForestClassifier(n_estimators=300, max_features=140, max_depth=2)\n",
    "RF_bag = BaggingClassifier(base_estimator=RF2, n_estimators=300, bootstrap=False, bootstrap_features=True, max_samples=100, max_features=200)\n",
    "\n",
    "accuracies = cross_val_score(estimator=RF_bag, X=X_scaled, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.585143035358\n",
      "Std.Dev of Accuracy:  0.0378362319751\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=2, max_features=140, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          learning_rate=0.1, n_estimators=300, random_state=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=300, max_features=140, max_depth=2)\n",
    "RF_ada = AdaBoostClassifier(base_estimator=RF, algorithm='SAMME.R',n_estimators=300, learning_rate=0.1)\n",
    "RF_ada.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.851265822785\n",
      "Test Accuracy:  0.617647058824\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy: ', RF_ada.score(X_train, y_train))\n",
    "print('Test Accuracy: ', RF_ada.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the adaptive boosting only slightly imporved the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first use the baseline model and benchmark its mean accuracy and standard deviation after cross-validation. Then we can try to tune the hyperparameters to get a better score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.726318918202\n",
      "Std.Dev of Accuracy: 0.0259648678389\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "GB_baseline = GradientBoostingClassifier()\n",
    "\n",
    "accuracies = cross_val_score(estimator=GB_baseline, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Std.Dev of Accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good model, as compared to previous ones. Let's try to improve it further by tweaking the most significant parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.762605140897\n",
      "Std.Dev of Accuracy: 0.0364546994011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "GB_baseline2 = GradientBoostingClassifier(min_samples_leaf=9, learning_rate=0.05, n_estimators=100)\n",
    "\n",
    "accuracies = cross_val_score(estimator=GB_baseline2, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Std.Dev of Accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the gradient boosting algorithm has the best bias-variance tradeoff, achieving the lowest possible bias among all models, and an acceptable variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that, while the bagging and boosting techniques mentioned above are usually effective, most did not do much to improve the models. This is due to the three factors mentioned earlier:\n",
    "<u1>\n",
    "<li>Imbalance in target classes.</li>\n",
    "<li>High dimensionality of the data.</li>\n",
    "<li>Scarcity of observations.</li>\n",
    "</u1>\n",
    "<br>We will now perform principal components analysis (PCA) to reduce the dimensionality of our dataset, then apply the same algorithms. We expect to see better results in terms of bias and variance with this approach. It may not improve the simple implementation of our machine learning algorithms due to information loss, but it will allow for computationally expensive approaches (such as combining bagging and boosting together) that would take a long time to execute without PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models with Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAGDCAYAAABuj7cYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e36d4c908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "var_exp = []\n",
    "\n",
    "for i in range(1,120):\n",
    "    pca = PCA(n_components=i, svd_solver='auto')\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    var_exp.append(pca.explained_variance_ratio_.sum())\n",
    "    \n",
    "var_exp_perc = [i*100 for i in var_exp]\n",
    "    \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1,120),var_exp_perc,color='blue', linestyle='dashed')\n",
    "plt.title('% of Variance Explained vs. Dimensions')\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('% of Variance Explained')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 100 dimensions is a good choice, as ~100% of the variance is explained with this number of dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100, svd_solver='auto')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_comb_pca = np.concatenate((X_train_pca, X_test_pca), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor Classifier with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "kFold = StratifiedKFold(n_splits=8)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn_pca = BaggingClassifier(base_estimator=knn, n_estimators=300,bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=bag_knn_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.544587367769\n",
      "Std.Dev of Accuracy:  0.0339289227454\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor Classifier with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "paste_knn_pca = BaggingClassifier(base_estimator=knn, n_estimators=300, bootstrap=False,bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=paste_knn_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.544587367769\n",
      "Std.Dev of Accuracy:  0.0339289227454\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LR = LogisticRegression(C=1, multi_class='multinomial', solver='lbfgs')\n",
    "LR_bag_pca = BaggingClassifier(base_estimator=LR, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LR_bag_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.544587367769\n",
      "Std.Dev of Accuracy:  0.0339289227454\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LR = LogisticRegression(C=1, multi_class='multinomial', solver='lbfgs')\n",
    "LR_paste_pca = BaggingClassifier(base_estimator=LR, n_estimators=300, bootstrap=False,bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LR_paste_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.544587367769\n",
      "Std.Dev of Accuracy:  0.0339289227454\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.768987341772\n",
      "Test Accuracy:  0.595588235294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "LR_ada = AdaBoostClassifier(base_estimator=LR, algorithm='SAMME',n_estimators=300, learning_rate=0.5)\n",
    "LR_ada.fit(X_train_pca, y_train)\n",
    "print('Train Accuracy: ', LR_ada.score(X_train_pca, y_train))\n",
    "print('Test Accuracy: ', LR_ada.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the test accuracy, but the model still has high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LSVC = LinearSVC(C=0.01, multi_class='crammer_singer')\n",
    "LSVC_bag_pca = BaggingClassifier(base_estimator=LSVC, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LSVC_bag_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.5293197821\n",
      "Std.Dev of Accuracy:  0.0218244262426\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine Classifier with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LSVC = LinearSVC(C=0.01, multi_class='crammer_singer')\n",
    "LSVC_paste_pca = BaggingClassifier(base_estimator=LSVC, n_estimators=300, bootstrap=False, bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=LSVC_paste_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.526994200705\n",
      "Std.Dev of Accuracy:  0.0176014273875\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine with Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC = LinearSVC(C=0.01, multi_class='crammer_singer')\n",
    "\n",
    "LR_ada = AdaBoostClassifier(base_estimator=LR, algorithm='SAMME',n_estimators=300, learning_rate=0.5)\n",
    "LR_ada.fit(X_train_pca, y_train)\n",
    "print('Train Accuracy: ', LR_ada.score(X_train_pca, y_train))\n",
    "print('Test Accuracy: ', LR_ada.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machine with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SVC = SVC(C=0.0001, gamma=1, kernel='poly')\n",
    "SVC_OVR = OneVsRestClassifier(estimator=SVC)\n",
    "SVC_bag_pca = BaggingClassifier(base_estimator=SVC_OVR, n_estimators=100, bootstrap_features=True, max_samples=200, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=SVC_bag_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelized Support Vector Machine with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "SVC = SVC(C=0.0001, gamma=1, kernel='poly')\n",
    "SVC_OVR = OneVsRestClassifier(estimator=SVC)\n",
    "SVC_paste_pca = BaggingClassifier(base_estimator=SVC_OVR, n_estimators=100, bootstrap=False,bootstrap_features=True, max_samples=200, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=SVC_paste_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DTF = DecisionTreeClassifier(min_samples_leaf=10, min_impurity_split=5)\n",
    "DTF_bag_pca = BaggingClassifier(base_estimator=DTF, n_estimators=300, bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=DTF_bag_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree with Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "DTF = DecisionTreeClassifier(min_samples_leaf=10, min_impurity_split=5)\n",
    "DTF_paste_pca = BaggingClassifier(base_estimator=DTF, n_estimators=300, bootstrap=False,bootstrap_features=True, max_samples=100, max_features=60)\n",
    "\n",
    "accuracies = cross_val_score(estimator=DTF_paste_pca, X=X_comb_pca, y=y, scoring='accuracy', cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy:  0.542510843787\n",
      "Std.Dev of Accuracy:  0.0162029371765\n"
     ]
    }
   ],
   "source": [
    "print('Mean Accuracy: ', accuracies.mean())\n",
    "print('Std.Dev of Accuracy: ', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_features': [18, 22, 25, 30], 'max_depth': [1, 2, 3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=300)\n",
    "\n",
    "params = {'max_features': [18,22,25,30],\n",
    "          'max_depth': [1,2,3]}\n",
    "\n",
    "RF_grid = GridSearchCV(estimator=RF, param_grid=params, cv=5, n_jobs=-1)\n",
    "RF_grid.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_max_features</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.947860</td>\n",
       "      <td>0.025060</td>\n",
       "      <td>0.566456</td>\n",
       "      <td>0.634558</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 25}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.536232</td>\n",
       "      <td>0.647773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.617188</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.643411</td>\n",
       "      <td>0.016548</td>\n",
       "      <td>9.059691e-03</td>\n",
       "      <td>0.037816</td>\n",
       "      <td>0.010667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341688</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 18}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>5.068366e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498662</td>\n",
       "      <td>0.027826</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 22}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.135743</td>\n",
       "      <td>6.128038e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.621883</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 25}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.660290</td>\n",
       "      <td>0.028126</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.557074</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 1, 'max_features': 30}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>6.250191e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.691178</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.560221</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 18}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.563492</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.550388</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>5.560829e-07</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.006508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.740634</td>\n",
       "      <td>0.031251</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.563325</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 22}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>4.156970e-07</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.003339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788646</td>\n",
       "      <td>0.028125</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.565728</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 25}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.570850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.558594</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.562016</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>6.249881e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.004687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.837789</td>\n",
       "      <td>0.028126</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.566441</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 2, 'max_features': 30}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.566802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.577519</td>\n",
       "      <td>0.005454</td>\n",
       "      <td>6.249690e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.807162</td>\n",
       "      <td>0.028126</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.626666</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 18}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.613281</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.007655</td>\n",
       "      <td>6.250143e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.008887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.881622</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.633700</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 22}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.646825</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.632812</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.631783</td>\n",
       "      <td>0.007368</td>\n",
       "      <td>7.654675e-03</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.861019</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.556962</td>\n",
       "      <td>0.646292</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>{'max_depth': 3, 'max_features': 30}</td>\n",
       "      <td>2</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.639676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546875</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.644531</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.655039</td>\n",
       "      <td>0.072966</td>\n",
       "      <td>3.504023e-07</td>\n",
       "      <td>0.029843</td>\n",
       "      <td>0.007393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "10       0.947860         0.025060         0.566456          0.634558   \n",
       "0        0.341688         0.012233         0.556962          0.557074   \n",
       "1        0.498662         0.027826         0.556962          0.557074   \n",
       "2        0.621883         0.031250         0.556962          0.557074   \n",
       "3        0.660290         0.028126         0.556962          0.557074   \n",
       "4        0.691178         0.031251         0.556962          0.560221   \n",
       "5        0.740634         0.031251         0.556962          0.563325   \n",
       "6        0.788646         0.028125         0.556962          0.565728   \n",
       "7        0.837789         0.028126         0.556962          0.566441   \n",
       "8        0.807162         0.028126         0.556962          0.626666   \n",
       "9        0.881622         0.025000         0.556962          0.633700   \n",
       "11       0.861019         0.015625         0.556962          0.646292   \n",
       "\n",
       "   param_max_depth param_max_features                                params  \\\n",
       "10               3                 25  {'max_depth': 3, 'max_features': 25}   \n",
       "0                1                 18  {'max_depth': 1, 'max_features': 18}   \n",
       "1                1                 22  {'max_depth': 1, 'max_features': 22}   \n",
       "2                1                 25  {'max_depth': 1, 'max_features': 25}   \n",
       "3                1                 30  {'max_depth': 1, 'max_features': 30}   \n",
       "4                2                 18  {'max_depth': 2, 'max_features': 18}   \n",
       "5                2                 22  {'max_depth': 2, 'max_features': 22}   \n",
       "6                2                 25  {'max_depth': 2, 'max_features': 25}   \n",
       "7                2                 30  {'max_depth': 2, 'max_features': 30}   \n",
       "8                3                 18  {'max_depth': 3, 'max_features': 18}   \n",
       "9                3                 22  {'max_depth': 3, 'max_features': 22}   \n",
       "11               3                 30  {'max_depth': 3, 'max_features': 30}   \n",
       "\n",
       "    rank_test_score  split0_test_score  split0_train_score       ...         \\\n",
       "10                1           0.536232            0.647773       ...          \n",
       "0                 2           0.521739            0.566802       ...          \n",
       "1                 2           0.521739            0.566802       ...          \n",
       "2                 2           0.521739            0.566802       ...          \n",
       "3                 2           0.521739            0.566802       ...          \n",
       "4                 2           0.521739            0.566802       ...          \n",
       "5                 2           0.521739            0.566802       ...          \n",
       "6                 2           0.521739            0.570850       ...          \n",
       "7                 2           0.521739            0.566802       ...          \n",
       "8                 2           0.521739            0.639676       ...          \n",
       "9                 2           0.521739            0.631579       ...          \n",
       "11                2           0.521739            0.639676       ...          \n",
       "\n",
       "    split2_test_score  split2_train_score  split3_test_score  \\\n",
       "10           0.546875            0.630952           0.583333   \n",
       "0            0.546875            0.559524           0.583333   \n",
       "1            0.546875            0.559524           0.583333   \n",
       "2            0.546875            0.559524           0.583333   \n",
       "3            0.546875            0.559524           0.583333   \n",
       "4            0.546875            0.563492           0.583333   \n",
       "5            0.546875            0.567460           0.583333   \n",
       "6            0.546875            0.567460           0.583333   \n",
       "7            0.546875            0.567460           0.583333   \n",
       "8            0.546875            0.630952           0.583333   \n",
       "9            0.546875            0.646825           0.583333   \n",
       "11           0.546875            0.654762           0.583333   \n",
       "\n",
       "    split3_train_score  split4_test_score  split4_train_score  std_fit_time  \\\n",
       "10            0.617188           0.637931            0.643411      0.016548   \n",
       "0             0.550781           0.603448            0.546512      0.008323   \n",
       "1             0.550781           0.603448            0.546512      0.135743   \n",
       "2             0.550781           0.603448            0.546512      0.006250   \n",
       "3             0.550781           0.603448            0.546512      0.006250   \n",
       "4             0.554688           0.603448            0.550388      0.006445   \n",
       "5             0.558594           0.603448            0.562016      0.012500   \n",
       "6             0.558594           0.603448            0.562016      0.007090   \n",
       "7             0.554688           0.603448            0.577519      0.005454   \n",
       "8             0.613281           0.603448            0.627907      0.007655   \n",
       "9             0.632812           0.603448            0.631783      0.007368   \n",
       "11            0.644531           0.603448            0.655039      0.072966   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "10    9.059691e-03        0.037816         0.010667  \n",
       "0     5.068366e-03        0.029843         0.007398  \n",
       "1     6.128038e-03        0.029843         0.007398  \n",
       "2     0.000000e+00        0.029843         0.007398  \n",
       "3     6.250191e-03        0.029843         0.007398  \n",
       "4     5.560829e-07        0.029843         0.006508  \n",
       "5     4.156970e-07        0.029843         0.003339  \n",
       "6     6.249881e-03        0.029843         0.004687  \n",
       "7     6.249690e-03        0.029843         0.007246  \n",
       "8     6.250143e-03        0.029843         0.008887  \n",
       "9     7.654675e-03        0.029843         0.007050  \n",
       "11    3.504023e-07        0.029843         0.007393  \n",
       "\n",
       "[12 rows x 22 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_grid_results = pd.DataFrame(RF_grid.cv_results_).sort_values('rank_test_score')\n",
    "RF_grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.524641336864\n",
      "Std.Dev of Accuracy: 0.0393769421816\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "GB_baseline2 = GradientBoostingClassifier(min_samples_leaf=9, learning_rate=0.05, n_estimators=100)\n",
    "\n",
    "accuracies = cross_val_score(estimator=GB_baseline2, X=X_comb_pca, y=y, cv=kFold, n_jobs=-1)\n",
    "print('Mean Accuracy:', accuracies.mean())\n",
    "print('Std.Dev of Accuracy:', accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models all perform poorly compared to the unreduced models in the previous section. This is largely due to scarcity of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do a final sanity check on our top two models (bagged LSVC and Gradient Boost). We will also check their classificaiton reports and confusion matrices to analyze the details of how well they performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagged LSVC Test Accuracy: 0.676470588235\n"
     ]
    }
   ],
   "source": [
    "LSVC_bag.fit(X_train, y_train)\n",
    "print('Bagged LSVC Test Accuracy:', LSVC_bag.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for bagged LSVC:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.96      0.79        69\n",
      "          2       0.56      0.45      0.50        11\n",
      "          3       1.00      1.00      1.00         7\n",
      "          4       0.33      0.33      0.33         3\n",
      "          5       0.00      0.00      0.00         4\n",
      "          6       0.00      0.00      0.00        10\n",
      "          8       0.00      0.00      0.00         1\n",
      "          9       0.50      1.00      0.67         1\n",
      "         10       0.75      0.63      0.69        19\n",
      "         14       0.00      0.00      0.00         3\n",
      "         15       0.00      0.00      0.00         1\n",
      "         16       0.00      0.00      0.00         7\n",
      "\n",
      "avg / total       0.55      0.68      0.60       136\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for bagged LSVC:\n",
      "[[66  1  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 6  5  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  7  0  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 3  0  0  0  0  0  0  0  1  0  0  0]\n",
      " [ 9  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 6  1  0  0  0  0  0  0 12  0  0  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 4  1  0  0  0  0  0  1  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Classification Report for bagged LSVC:')\n",
    "print(classification_report(y_true = y_test, y_pred= LSVC_bag.predict(X_test)))\n",
    "print('\\n')\n",
    "print('Confusion Matrix for bagged LSVC:')\n",
    "print(confusion_matrix(y_true = y_test, y_pred= LSVC_bag.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GB Test Accuracy: 0.691176470588\n"
     ]
    }
   ],
   "source": [
    "GB_baseline2.fit(X_train, y_train)\n",
    "print('GB Test Accuracy:', GB_baseline2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for GB:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.70      0.94      0.80        69\n",
      "          2       0.67      0.55      0.60        11\n",
      "          3       1.00      0.29      0.44         7\n",
      "          4       0.33      0.33      0.33         3\n",
      "          5       0.75      0.75      0.75         4\n",
      "          6       0.88      0.70      0.78        10\n",
      "          8       0.00      0.00      0.00         1\n",
      "          9       1.00      1.00      1.00         1\n",
      "         10       0.75      0.47      0.58        19\n",
      "         14       0.00      0.00      0.00         3\n",
      "         15       0.00      0.00      0.00         1\n",
      "         16       0.00      0.00      0.00         7\n",
      "\n",
      "avg / total       0.67      0.69      0.65       136\n",
      "\n",
      "\n",
      "\n",
      "Confusion Matrix for GB:\n",
      "[[65  1  0  1  0  0  0  0  1  0  0  1]\n",
      " [ 4  6  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 3  0  2  0  1  0  0  0  0  0  0  1]\n",
      " [ 1  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 1  0  0  0  3  0  0  0  0  0  0  0]\n",
      " [ 2  0  0  1  0  7  0  0  0  0  0  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 8  1  0  0  0  0  0  0  9  0  1  0]\n",
      " [ 2  0  0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  1  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 6  0  0  0  0  0  0  0  1  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print('Classification Report for GB:')\n",
    "print(classification_report(y_true = y_test, y_pred= GB_baseline2.predict(X_test)))\n",
    "print('\\n')\n",
    "print('Confusion Matrix for GB:')\n",
    "print(confusion_matrix(y_true = y_test, y_pred= GB_baseline2.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the following are the reasons that the models did not perform well even with dimensionality reduction, bagging/boosting methods, and GridSearch:\n",
    "- Scarcity of data: the dataset has too few records (452)\n",
    "- High dimensionality: the dataset has too many features (280), none of which having a strong correlation with the target variable.\n",
    "- Imbalance in target classes: The majority of records are of the 'Normal' class, which biases the dataset. Oversampling/undersampling methods would not work. Oversampling would create too many synthetic observations, which might cause the models to overfit. Undersampling will lead to too few records. This is why bagging was the best option to deal with this problem, which also did not help fix this issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for this data would have to the Gradient Boosting, achieving an accuracy of 69.1% on the test set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
